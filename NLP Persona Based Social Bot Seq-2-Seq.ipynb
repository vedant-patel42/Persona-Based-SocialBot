{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1648432818860,"user":{"displayName":"Om Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05317090667729904210"},"user_tz":240},"id":"fkaxWOvJmrPm","outputId":"207ec03d-c1e6-49d4-a399-c6478106bdda"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}],"source":["%cd /content"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NfLRmNVV_Ndu"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4882,"status":"ok","timestamp":1648414493911,"user":{"displayName":"Rushabh Shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04019200460865043006"},"user_tz":240},"id":"oTG0q-g6FIm2","outputId":"44da4a11-46bf-4b92-ee4b-de60a097c966"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'ParlAI'...\n","remote: Enumerating objects: 14709, done.\u001b[K\n","remote: Total 14709 (delta 0), reused 0 (delta 0), pack-reused 14709\u001b[K\n","Receiving objects: 100% (14709/14709), 12.03 MiB | 4.14 MiB/s, done.\n","Resolving deltas: 100% (10132/10132), done.\n"]}],"source":["\n","!git clone -b convai https://github.com/pgoel92/ParlAI.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":56578,"status":"ok","timestamp":1648433298095,"user":{"displayName":"Om Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05317090667729904210"},"user_tz":240},"id":"s0k5v0ijkJRR","outputId":"fc89f7fb-d712-41b1-ce3b-351b5a23560a"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","Collecting parlai\n","  Downloading parlai-1.5.1-py3-none-any.whl (1.5 MB)\n","\u001b[K     |████████████████████████████████| 1.5 MB 4.3 MB/s \n","\u001b[?25hCollecting datasets>=1.4.1\n","  Downloading datasets-2.0.0-py3-none-any.whl (325 kB)\n","\u001b[K     |████████████████████████████████| 325 kB 48.8 MB/s \n","\u001b[?25hCollecting py-gfm\n","  Downloading py_gfm-1.0.2-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from parlai) (3.13)\n","Collecting sh\n","  Downloading sh-1.14.2-py2.py3-none-any.whl (40 kB)\n","\u001b[K     |████████████████████████████████| 40 kB 5.9 MB/s \n","\u001b[?25hCollecting coloredlogs\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[K     |████████████████████████████████| 46 kB 3.7 MB/s \n","\u001b[?25hCollecting myst-parser~=0.12.2\n","  Downloading myst_parser-0.12.10-py3-none-any.whl (34 kB)\n","Collecting docutils<0.16,>=0.14\n","  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n","\u001b[K     |████████████████████████████████| 547 kB 41.4 MB/s \n","\u001b[?25hCollecting importlib-metadata<4.3\n","  Downloading importlib_metadata-4.2.0-py3-none-any.whl (16 kB)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from parlai) (1.0.2)\n","Collecting gitdb2\n","  Downloading gitdb2-4.0.2-py3-none-any.whl (1.1 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from parlai) (1.3.5)\n","Collecting fairscale\n","  Downloading fairscale-0.4.6.tar.gz (248 kB)\n","\u001b[K     |████████████████████████████████| 248 kB 44.7 MB/s \n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Collecting Sphinx~=2.2.0\n","  Downloading Sphinx-2.2.2-py3-none-any.whl (2.7 MB)\n","\u001b[K     |████████████████████████████████| 2.7 MB 40.9 MB/s \n","\u001b[?25hCollecting omegaconf~=2.1.1\n","  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n","\u001b[K     |████████████████████████████████| 74 kB 3.3 MB/s \n","\u001b[?25hRequirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from parlai) (4.8.0)\n","Collecting flake8\n","  Downloading flake8-4.0.1-py2.py3-none-any.whl (64 kB)\n","\u001b[K     |████████████████████████████████| 64 kB 2.4 MB/s \n","\u001b[?25hCollecting tensorboardX\n","  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n","\u001b[K     |████████████████████████████████| 125 kB 51.5 MB/s \n","\u001b[?25hCollecting emoji\n","  Downloading emoji-1.7.0.tar.gz (175 kB)\n","\u001b[K     |████████████████████████████████| 175 kB 50.7 MB/s \n","\u001b[?25hRequirement already satisfied: tornado in /usr/local/lib/python3.7/dist-packages (from parlai) (5.1.1)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from parlai) (3.6.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from parlai) (1.21.5)\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from parlai) (1.10.0+cu111)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from parlai) (1.4.1)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from parlai) (5.5.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from parlai) (1.1.0)\n","Collecting urllib3>=1.26.5\n","  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n","\u001b[K     |████████████████████████████████| 138 kB 34.5 MB/s \n","\u001b[?25hCollecting tqdm~=4.62.1\n","  Downloading tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n","\u001b[K     |████████████████████████████████| 76 kB 4.6 MB/s \n","\u001b[?25hCollecting jsonlines\n","  Downloading jsonlines-3.0.0-py3-none-any.whl (8.5 kB)\n","Collecting GitPython\n","  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n","\u001b[K     |████████████████████████████████| 181 kB 46.7 MB/s \n","\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from parlai) (7.1.2)\n","Collecting regex>=2021.8.3\n","  Downloading regex-2022.3.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (749 kB)\n","\u001b[K     |████████████████████████████████| 749 kB 44.9 MB/s \n","\u001b[?25hCollecting iopath~=0.1.8\n","  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from parlai) (3.10.0.2)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from parlai) (3.2.5)\n","Requirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from parlai) (22.3.0)\n","Collecting py-rouge\n","  Downloading py_rouge-1.1-py3-none-any.whl (56 kB)\n","\u001b[K     |████████████████████████████████| 56 kB 4.7 MB/s \n","\u001b[?25hRequirement already satisfied: torchtext>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from parlai) (0.11.0)\n","Collecting subword-nmt\n","  Downloading subword_nmt-0.3.8-py3-none-any.whl (27 kB)\n","Collecting websocket-client\n","  Downloading websocket_client-1.3.1-py3-none-any.whl (54 kB)\n","\u001b[K     |████████████████████████████████| 54 kB 2.6 MB/s \n","\u001b[?25hCollecting tokenizers>=0.8.0\n","  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n","\u001b[K     |████████████████████████████████| 6.5 MB 33.1 MB/s \n","\u001b[?25hCollecting docformatter\n","  Downloading docformatter-1.4.tar.gz (208 kB)\n","\u001b[K     |████████████████████████████████| 208 kB 49.4 MB/s \n","\u001b[?25hCollecting Unidecode\n","  Downloading Unidecode-1.3.4-py3-none-any.whl (235 kB)\n","\u001b[K     |████████████████████████████████| 235 kB 34.1 MB/s \n","\u001b[?25hCollecting websocket-server\n","  Downloading websocket_server-0.6.4-py3-none-any.whl (7.5 kB)\n","Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from parlai) (2.8.0)\n","Collecting boto3\n","  Downloading boto3-1.21.27-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 50.9 MB/s \n","\u001b[?25hCollecting botocore\n","  Downloading botocore-1.24.27-py3-none-any.whl (8.6 MB)\n","\u001b[K     |████████████████████████████████| 8.6 MB 38.1 MB/s \n","\u001b[?25hCollecting hydra-core~=1.1.0\n","  Downloading hydra_core-1.1.1-py3-none-any.whl (145 kB)\n","\u001b[K     |████████████████████████████████| 145 kB 49.1 MB/s \n","\u001b[?25hCollecting attrs~=20.2.0\n","  Downloading attrs-20.2.0-py2.py3-none-any.whl (48 kB)\n","\u001b[K     |████████████████████████████████| 48 kB 3.6 MB/s \n","\u001b[?25hCollecting pytest-regressions\n","  Downloading pytest_regressions-2.3.1-py3-none-any.whl (22 kB)\n","Collecting requests-mock\n","  Downloading requests_mock-1.9.3-py2.py3-none-any.whl (27 kB)\n","Collecting sphinx-rtd-theme\n","  Downloading sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl (2.8 MB)\n","\u001b[K     |████████████████████████████████| 2.8 MB 26.9 MB/s \n","\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from parlai) (2.23.0)\n","Collecting flake8-bugbear\n","  Downloading flake8_bugbear-22.3.23-py3-none-any.whl (19 kB)\n","Collecting sphinx-autodoc-typehints~=1.10.3\n","  Downloading sphinx_autodoc_typehints-1.10.3-py3-none-any.whl (8.4 kB)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets>=1.4.1->parlai) (0.70.12.2)\n","Requirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=1.4.1->parlai) (6.0.1)\n","Collecting fsspec[http]>=2021.05.0\n","  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n","\u001b[K     |████████████████████████████████| 134 kB 50.8 MB/s \n","\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets>=1.4.1->parlai) (0.3.4)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 47.5 MB/s \n","\u001b[?25hCollecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets>=1.4.1->parlai) (21.3)\n","Collecting xxhash\n","  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 50.1 MB/s \n","\u001b[?25hCollecting huggingface-hub<1.0.0,>=0.1.0\n","  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n","\u001b[K     |████████████████████████████████| 67 kB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.4.1->parlai) (3.6.0)\n","Collecting antlr4-python3-runtime==4.8\n","  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n","\u001b[K     |████████████████████████████████| 112 kB 43.0 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core~=1.1.0->parlai) (5.4.0)\n","Collecting pyyaml\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 44.9 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.3->parlai) (3.7.0)\n","Collecting portalocker\n","  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n","Collecting markdown-it-py~=0.5.4\n","  Downloading markdown_it_py-0.5.8-py3-none-any.whl (110 kB)\n","\u001b[K     |████████████████████████████████| 110 kB 50.0 MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets>=1.4.1->parlai) (3.0.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->parlai) (2.10)\n","Collecting requests<3,>=2.21.0\n","  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 1.6 MB/s \n","\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->parlai) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->parlai) (2021.10.8)\n","Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from Sphinx~=2.2.0->parlai) (2.6.1)\n","Collecting sphinxcontrib-qthelp\n","  Downloading sphinxcontrib_qthelp-1.0.3-py2.py3-none-any.whl (90 kB)\n","\u001b[K     |████████████████████████████████| 90 kB 8.6 MB/s \n","\u001b[?25hCollecting sphinxcontrib-devhelp\n","  Downloading sphinxcontrib_devhelp-1.0.2-py2.py3-none-any.whl (84 kB)\n","\u001b[K     |████████████████████████████████| 84 kB 3.6 MB/s \n","\u001b[?25hRequirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from Sphinx~=2.2.0->parlai) (2.2.0)\n","Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from Sphinx~=2.2.0->parlai) (2.11.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from Sphinx~=2.2.0->parlai) (57.4.0)\n","Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from Sphinx~=2.2.0->parlai) (1.3.0)\n","Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from Sphinx~=2.2.0->parlai) (1.1.5)\n","Collecting sphinxcontrib-htmlhelp\n","  Downloading sphinxcontrib_htmlhelp-2.0.0-py2.py3-none-any.whl (100 kB)\n","\u001b[K     |████████████████████████████████| 100 kB 9.9 MB/s \n","\u001b[?25hCollecting sphinxcontrib-applehelp\n","  Downloading sphinxcontrib_applehelp-1.0.2-py2.py3-none-any.whl (121 kB)\n","\u001b[K     |████████████████████████████████| 121 kB 48.4 MB/s \n","\u001b[?25hRequirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from Sphinx~=2.2.0->parlai) (2.9.1)\n","Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from Sphinx~=2.2.0->parlai) (0.7.12)\n","Collecting sphinxcontrib-jsmath\n","  Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n","Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel!=2.0,>=1.3->Sphinx~=2.2.0->parlai) (2018.9)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->Sphinx~=2.2.0->parlai) (2.0.1)\n","Collecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n","\u001b[K     |████████████████████████████████| 144 kB 51.7 MB/s \n","\u001b[?25hCollecting asynctest==0.13.0\n","  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n","\u001b[K     |████████████████████████████████| 271 kB 48.1 MB/s \n","\u001b[?25hCollecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 2.2 MB/s \n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting s3transfer<0.6.0,>=0.5.0\n","  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 8.3 MB/s \n","\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n","  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore->parlai) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore->parlai) (1.15.0)\n","Collecting humanfriendly>=9.1\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 5.8 MB/s \n","\u001b[?25hCollecting untokenize\n","  Downloading untokenize-0.1.1.tar.gz (3.1 kB)\n","Collecting pycodestyle<2.9.0,>=2.8.0\n","  Downloading pycodestyle-2.8.0-py2.py3-none-any.whl (42 kB)\n","\u001b[K     |████████████████████████████████| 42 kB 860 kB/s \n","\u001b[?25hCollecting pyflakes<2.5.0,>=2.4.0\n","  Downloading pyflakes-2.4.0-py2.py3-none-any.whl (69 kB)\n","\u001b[K     |████████████████████████████████| 69 kB 8.3 MB/s \n","\u001b[?25hCollecting mccabe<0.7.0,>=0.6.0\n","  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n","Collecting gitdb>=4.0.1\n","  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n","\u001b[?25hCollecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->parlai) (0.7.5)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->parlai) (1.0.18)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->parlai) (4.4.2)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->parlai) (5.1.1)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->parlai) (0.8.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->parlai) (0.2.5)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->parlai) (0.7.0)\n","Requirement already satisfied: markdown~=3.2 in /usr/local/lib/python3.7/dist-packages (from py-gfm->parlai) (3.3.6)\n","Collecting markdown~=3.2\n","  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n","\u001b[K     |████████████████████████████████| 97 kB 6.8 MB/s \n","\u001b[?25hRequirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->parlai) (1.11.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->parlai) (1.4.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->parlai) (0.7.1)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->parlai) (8.12.0)\n","Collecting pytest-datadir>=1.2.0\n","  Downloading pytest_datadir-1.3.1-py2.py3-none-any.whl (5.9 kB)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->parlai) (3.1.0)\n","Collecting mock\n","  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->parlai) (3.17.3)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->parlai) (0.37.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->parlai) (1.0.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->parlai) (1.8.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->parlai) (0.4.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->parlai) (0.6.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->parlai) (1.0.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->parlai) (1.35.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->parlai) (1.44.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->parlai) (4.8)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->parlai) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->parlai) (4.2.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->parlai) (1.3.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->parlai) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->parlai) (3.2.0)\n","Building wheels for collected packages: antlr4-python3-runtime, docformatter, emoji, fairscale, untokenize\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=95f2f762b554c31b212b0c3f6fc83429031cdd9846ffecb0a9fb92b27c02e922\n","  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n","  Building wheel for docformatter (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docformatter: filename=docformatter-1.4-py3-none-any.whl size=12405 sha256=50673aa4e178fbb977e7f5e24b974bf974c136719080fc47d1ae80a84270bcc7\n","  Stored in directory: /root/.cache/pip/wheels/f7/62/81/a91b835e57388111fbe5f8c0cc99c5be6e257ba1fda172dd19\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=43b839f135a601ddacfa58dce870d438eb5cbe8b9d58b2d730b583c5175524a3\n","  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n","  Building wheel for fairscale (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fairscale: filename=fairscale-0.4.6-py3-none-any.whl size=307252 sha256=32781243ea48e6e526d37282b27803697e87194131fa2a2a31b10dc262a068f6\n","  Stored in directory: /root/.cache/pip/wheels/4e/4f/0b/94c29ea06dfad93260cb0377855f87b7b863312317a7f69fe7\n","  Building wheel for untokenize (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for untokenize: filename=untokenize-0.1.1-py3-none-any.whl size=2888 sha256=3d9436e5a491be919d05833a5064956d6a5eefa528729a47b1dd49693da091f0\n","  Stored in directory: /root/.cache/pip/wheels/5e/d4/22/3b662355e9a2faa5fe462c17b6fae2e9757066c36cd72c4497\n","Successfully built antlr4-python3-runtime docformatter emoji fairscale untokenize\n","Installing collected packages: urllib3, multidict, frozenlist, yarl, requests, jmespath, attrs, asynctest, async-timeout, aiosignal, tqdm, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, smmap, pyyaml, pyflakes, pycodestyle, mccabe, importlib-metadata, fsspec, docutils, botocore, antlr4-python3-runtime, aiohttp, xxhash, untokenize, Sphinx, s3transfer, responses, pytest-datadir, portalocker, omegaconf, mock, markdown-it-py, markdown, humanfriendly, huggingface-hub, gitdb, flake8, websocket-server, websocket-client, Unidecode, tokenizers, tensorboardX, subword-nmt, sphinx-rtd-theme, sphinx-autodoc-typehints, sh, requests-mock, regex, pytest-regressions, py-rouge, py-gfm, myst-parser, jsonlines, iopath, hydra-core, GitPython, gitdb2, flake8-bugbear, fairscale, emoji, docformatter, datasets, coloredlogs, boto3, parlai\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","  Attempting uninstall: attrs\n","    Found existing installation: attrs 21.4.0\n","    Uninstalling attrs-21.4.0:\n","      Successfully uninstalled attrs-21.4.0\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.63.0\n","    Uninstalling tqdm-4.63.0:\n","      Successfully uninstalled tqdm-4.63.0\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Attempting uninstall: importlib-metadata\n","    Found existing installation: importlib-metadata 4.11.3\n","    Uninstalling importlib-metadata-4.11.3:\n","      Successfully uninstalled importlib-metadata-4.11.3\n","  Attempting uninstall: docutils\n","    Found existing installation: docutils 0.17.1\n","    Uninstalling docutils-0.17.1:\n","      Successfully uninstalled docutils-0.17.1\n","  Attempting uninstall: Sphinx\n","    Found existing installation: Sphinx 1.8.6\n","    Uninstalling Sphinx-1.8.6:\n","      Successfully uninstalled Sphinx-1.8.6\n","  Attempting uninstall: markdown\n","    Found existing installation: Markdown 3.3.6\n","    Uninstalling Markdown-3.3.6:\n","      Successfully uninstalled Markdown-3.3.6\n","  Attempting uninstall: regex\n","    Found existing installation: regex 2019.12.20\n","    Uninstalling regex-2019.12.20:\n","      Successfully uninstalled regex-2019.12.20\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed GitPython-3.1.27 Sphinx-2.2.2 Unidecode-1.3.4 aiohttp-3.8.1 aiosignal-1.2.0 antlr4-python3-runtime-4.8 async-timeout-4.0.2 asynctest-0.13.0 attrs-20.2.0 boto3-1.21.27 botocore-1.24.27 coloredlogs-15.0.1 datasets-2.0.0 docformatter-1.4 docutils-0.15.2 emoji-1.7.0 fairscale-0.4.6 flake8-4.0.1 flake8-bugbear-22.3.23 frozenlist-1.3.0 fsspec-2022.2.0 gitdb-4.0.9 gitdb2-4.0.2 huggingface-hub-0.4.0 humanfriendly-10.0 hydra-core-1.1.1 importlib-metadata-4.2.0 iopath-0.1.9 jmespath-1.0.0 jsonlines-3.0.0 markdown-3.3.4 markdown-it-py-0.5.8 mccabe-0.6.1 mock-4.0.3 multidict-6.0.2 myst-parser-0.12.10 omegaconf-2.1.1 parlai-1.5.1 portalocker-2.4.0 py-gfm-1.0.2 py-rouge-1.1 pycodestyle-2.8.0 pyflakes-2.4.0 pytest-datadir-1.3.1 pytest-regressions-2.3.1 pyyaml-6.0 regex-2022.3.15 requests-2.27.1 requests-mock-1.9.3 responses-0.18.0 s3transfer-0.5.2 sh-1.14.2 smmap-5.0.0 sphinx-autodoc-typehints-1.10.3 sphinx-rtd-theme-1.0.0 sphinxcontrib-applehelp-1.0.2 sphinxcontrib-devhelp-1.0.2 sphinxcontrib-htmlhelp-2.0.0 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-1.0.3 subword-nmt-0.3.8 tensorboardX-2.5 tokenizers-0.11.6 tqdm-4.62.3 untokenize-0.1.1 urllib3-1.26.9 websocket-client-1.3.1 websocket-server-0.6.4 xxhash-3.0.0 yarl-1.7.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pydevd_plugins","sphinxcontrib"]}}},"metadata":{}}],"source":["!pwd\n","!pip install parlai"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":235,"status":"ok","timestamp":1648414582103,"user":{"displayName":"Rushabh Shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04019200460865043006"},"user_tz":240},"id":"SK6f6AkCjgAV","outputId":"1a4d3592-5904-4bc3-858e-3003fae091b0"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Errno 2] No such file or directory: 'ParlAI/projects/convai2/pgoel/seq2seq/'\n","/bleurt\n"]}],"source":["%cd ParlAI/projects/convai2/pgoel/seq2seq/\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":598851,"status":"ok","timestamp":1648415182732,"user":{"displayName":"Rushabh Shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04019200460865043006"},"user_tz":240},"id":"BiTvDWpekpyA","outputId":"c97f461f-da85-463d-d43b-25aa2c08bbb7"},"outputs":[{"name":"stdout","output_type":"stream","text":["20:56:32 | building dictionary first...\n","20:56:32 | Opt:\n","20:56:32 |     adafactor_eps: '(1e-30, 0.001)'\n","20:56:32 |     adam_eps: 1e-08\n","20:56:32 |     add_p1_after_newln: False\n","20:56:32 |     aggregate_micro: False\n","20:56:32 |     allow_missing_init_opts: False\n","20:56:32 |     attention: general\n","20:56:32 |     attention_length: 48\n","20:56:32 |     attention_time: post\n","20:56:32 |     batchsize: 1\n","20:56:32 |     beam_block_full_context: True\n","20:56:32 |     beam_block_list_filename: None\n","20:56:32 |     beam_block_ngram: -1\n","20:56:32 |     beam_context_block_ngram: -1\n","20:56:32 |     beam_delay: 30\n","20:56:32 |     beam_length_penalty: 0.65\n","20:56:32 |     beam_min_length: 1\n","20:56:32 |     beam_size: 1\n","20:56:32 |     betas: '(0.9, 0.999)'\n","20:56:32 |     bidirectional: False\n","20:56:32 |     bpe_add_prefix_space: None\n","20:56:32 |     bpe_debug: False\n","20:56:32 |     bpe_dropout: None\n","20:56:32 |     bpe_merge: None\n","20:56:32 |     bpe_vocab: None\n","20:56:32 |     compute_tokenized_bleu: False\n","20:56:32 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n","20:56:32 |     datatype: train\n","20:56:32 |     decoder: same\n","20:56:32 |     delimiter: '\\n'\n","20:56:32 |     dict_class: parlai.core.dict:DictionaryAgent\n","20:56:32 |     dict_endtoken: __end__\n","20:56:32 |     dict_file: /tmp/myseq2seqmodel.dict\n","20:56:32 |     dict_include_test: False\n","20:56:32 |     dict_include_valid: True\n","20:56:32 |     dict_initpath: None\n","20:56:32 |     dict_language: english\n","20:56:32 |     dict_loaded: False\n","20:56:32 |     dict_lower: True\n","20:56:32 |     dict_max_ngram_size: -1\n","20:56:32 |     dict_maxexs: -1\n","20:56:32 |     dict_maxtokens: -1\n","20:56:32 |     dict_minfreq: 0\n","20:56:32 |     dict_nulltoken: __null__\n","20:56:32 |     dict_starttoken: __start__\n","20:56:32 |     dict_textfields: text,labels\n","20:56:32 |     dict_tokenizer: split\n","20:56:32 |     dict_unktoken: __unk__\n","20:56:32 |     display_examples: False\n","20:56:32 |     download_path: None\n","20:56:32 |     dropout: 0.2\n","20:56:32 |     dynamic_batching: None\n","20:56:32 |     embedding_projection: random\n","20:56:32 |     embedding_type: random\n","20:56:32 |     embeddingsize: 128\n","20:56:32 |     encoder: lstm\n","20:56:32 |     eval_batchsize: None\n","20:56:32 |     eval_dynamic_batching: None\n","20:56:32 |     evaltask: None\n","20:56:32 |     final_extra_opt: \n","20:56:32 |     force_fp16_tokens: False\n","20:56:32 |     fp16: False\n","20:56:32 |     fp16_impl: safe\n","20:56:32 |     gpu: -1\n","20:56:32 |     gradient_clip: 0.1\n","20:56:32 |     hiddensize: 1024\n","20:56:32 |     hide_labels: False\n","20:56:32 |     history_add_global_end_token: None\n","20:56:32 |     history_reversed: False\n","20:56:32 |     history_size: -1\n","20:56:32 |     image_cropsize: 224\n","20:56:32 |     image_mode: no_image_model\n","20:56:32 |     image_size: 256\n","20:56:32 |     inference: greedy\n","20:56:32 |     init_model: None\n","20:56:32 |     init_opt: None\n","20:56:32 |     input_dropout: 0.0\n","20:56:32 |     interactive_mode: False\n","20:56:32 |     invsqrt_lr_decay_gamma: -1\n","20:56:32 |     is_debug: False\n","20:56:32 |     label_truncate: None\n","20:56:32 |     learningrate: 1\n","20:56:32 |     load_from_checkpoint: True\n","20:56:32 |     log_every_n_secs: 10\n","20:56:32 |     log_every_n_steps: 50\n","20:56:32 |     loglevel: info\n","20:56:32 |     lookuptable: unique\n","20:56:32 |     lr_scheduler: reduceonplateau\n","20:56:32 |     lr_scheduler_decay: 0.5\n","20:56:32 |     lr_scheduler_patience: 3\n","20:56:32 |     max_train_steps: -1\n","20:56:32 |     max_train_time: -1\n","20:56:32 |     metrics: default\n","20:56:32 |     model: parlai.agents.seq2seq.seq2seq:Seq2seqAgent\n","20:56:32 |     model_file: /tmp/myseq2seqmodel\n","20:56:32 |     momentum: 0\n","20:56:32 |     multitask_weights: [1]\n","20:56:32 |     mutators: None\n","20:56:32 |     nesterov: True\n","20:56:32 |     no_cuda: False\n","20:56:32 |     num_epochs: 0.01\n","20:56:32 |     num_workers: 0\n","20:56:32 |     numlayers: 1\n","20:56:32 |     numsoftmax: 1\n","20:56:32 |     nus: (0.7,)\n","20:56:32 |     optimizer: sgd\n","20:56:32 |     override: \"{'num_epochs': 0.01, 'task': 'personachat'}\"\n","20:56:32 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n","20:56:32 |     person_tokens: False\n","20:56:32 |     personachat_attnsentlevel: True\n","20:56:32 |     personachat_learnreweight: True\n","20:56:32 |     personachat_reweight: use\n","20:56:32 |     personachat_sharelt: True\n","20:56:32 |     rank_candidates: True\n","20:56:32 |     rnn_class: lstm\n","20:56:32 |     save_after_valid: False\n","20:56:32 |     save_every_n_secs: 180\n","20:56:32 |     short_final_eval: False\n","20:56:32 |     skip_generation: False\n","20:56:32 |     special_tok_lst: None\n","20:56:32 |     split_lines: False\n","20:56:32 |     starttime: Mar27_20-56\n","20:56:32 |     task: personachat\n","20:56:32 |     temperature: 1.0\n","20:56:32 |     tensorboard_log: False\n","20:56:32 |     tensorboard_logdir: None\n","20:56:32 |     text_truncate: None\n","20:56:32 |     topk: 10\n","20:56:32 |     topp: 0.9\n","20:56:32 |     truncate: 100\n","20:56:32 |     update_freq: 1\n","20:56:32 |     use_reply: label\n","20:56:32 |     validation_cutoff: 1.0\n","20:56:32 |     validation_every_n_epochs: -1\n","20:56:32 |     validation_every_n_secs: 100\n","20:56:32 |     validation_every_n_steps: -1\n","20:56:32 |     validation_max_exs: -1\n","20:56:32 |     validation_metric: ppl\n","20:56:32 |     validation_metric_mode: max\n","20:56:32 |     validation_patience: 10\n","20:56:32 |     validation_share_agent: False\n","20:56:32 |     verbose: False\n","20:56:32 |     wandb_entity: None\n","20:56:32 |     wandb_log: False\n","20:56:32 |     wandb_name: None\n","20:56:32 |     wandb_project: None\n","20:56:32 |     warmup_rate: 0.0001\n","20:56:32 |     warmup_updates: -1\n","20:56:32 |     weight_decay: None\n","20:56:32 | Current internal commit: c4b46ccfdc0e6a03e35aefea8fc79f767c86e4ba\n","20:56:32 | Current fb commit: c4b46ccfdc0e6a03e35aefea8fc79f767c86e4ba\n","20:56:32 | creating task(s): personachat\n","[building data: /usr/local/lib/python3.7/dist-packages/data/Persona-Chat]\n","20:56:32 | Downloading http://parl.ai/downloads/personachat/personachat.tgz to /usr/local/lib/python3.7/dist-packages/data/Persona-Chat/personachat.tgz\n","Downloading personachat.tgz: 100% 223M/223M [00:12<00:00, 18.1MB/s]\n","20:57:01 | loading fbdialog data: /usr/local/lib/python3.7/dist-packages/data/Persona-Chat/personachat/train_self_original.txt\n","Building dictionary:   0% 0.00/65.7k [00:00<?, ?ex/s]20:57:02 | loading fbdialog data: /usr/local/lib/python3.7/dist-packages/data/Persona-Chat/personachat/train_self_original.txt\n","Building dictionary: 100% 65.7k/65.7k [00:04<00:00, 14.0kex/s]\n","20:57:07 | creating task(s): personachat\n","20:57:07 | loading fbdialog data: /usr/local/lib/python3.7/dist-packages/data/Persona-Chat/personachat/valid_self_original.txt\n","Building dictionary:   0% 0.00/7.80k [00:00<?, ?ex/s]20:57:07 | loading fbdialog data: /usr/local/lib/python3.7/dist-packages/data/Persona-Chat/personachat/valid_self_original.txt\n","Building dictionary: 100% 7.80k/7.80k [00:00<00:00, 17.0kex/s]\n","20:57:07 | Saving dictionary to /tmp/myseq2seqmodel.dict\n","20:57:07 | dictionary built with 19244 tokens in 0.0s\n","20:57:07 | No model with opt yet at: /tmp/myseq2seqmodel(.opt)\n","20:57:07 | Using CUDA\n","20:57:07 | loading dictionary from /tmp/myseq2seqmodel.dict\n","20:57:07 | num words = 19244\n","20:57:17 | Total parameters: 20,139,436 (20,139,436 trainable)\n","20:57:17 | Opt:\n","20:57:17 |     adafactor_eps: '(1e-30, 0.001)'\n","20:57:17 |     adam_eps: 1e-08\n","20:57:17 |     add_p1_after_newln: False\n","20:57:17 |     aggregate_micro: False\n","20:57:17 |     allow_missing_init_opts: False\n","20:57:17 |     attention: general\n","20:57:17 |     attention_length: 48\n","20:57:17 |     attention_time: post\n","20:57:17 |     batchsize: 128\n","20:57:17 |     beam_block_full_context: True\n","20:57:17 |     beam_block_list_filename: None\n","20:57:17 |     beam_block_ngram: -1\n","20:57:17 |     beam_context_block_ngram: -1\n","20:57:17 |     beam_delay: 30\n","20:57:17 |     beam_length_penalty: 0.65\n","20:57:17 |     beam_min_length: 1\n","20:57:17 |     beam_size: 1\n","20:57:17 |     betas: '(0.9, 0.999)'\n","20:57:17 |     bidirectional: False\n","20:57:17 |     bpe_add_prefix_space: None\n","20:57:17 |     bpe_debug: False\n","20:57:17 |     bpe_dropout: None\n","20:57:17 |     bpe_merge: None\n","20:57:17 |     bpe_vocab: None\n","20:57:17 |     compute_tokenized_bleu: False\n","20:57:17 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n","20:57:17 |     datatype: train\n","20:57:17 |     decoder: same\n","20:57:17 |     delimiter: '\\n'\n","20:57:17 |     dict_class: parlai.core.dict:DictionaryAgent\n","20:57:17 |     dict_endtoken: __end__\n","20:57:17 |     dict_file: /tmp/myseq2seqmodel.dict\n","20:57:17 |     dict_include_test: False\n","20:57:17 |     dict_include_valid: True\n","20:57:17 |     dict_initpath: None\n","20:57:17 |     dict_language: english\n","20:57:17 |     dict_loaded: True\n","20:57:17 |     dict_lower: True\n","20:57:17 |     dict_max_ngram_size: -1\n","20:57:17 |     dict_maxexs: -1\n","20:57:17 |     dict_maxtokens: -1\n","20:57:17 |     dict_minfreq: 0\n","20:57:17 |     dict_nulltoken: __null__\n","20:57:17 |     dict_starttoken: __start__\n","20:57:17 |     dict_textfields: text,labels\n","20:57:17 |     dict_tokenizer: split\n","20:57:17 |     dict_unktoken: __unk__\n","20:57:17 |     display_examples: False\n","20:57:17 |     download_path: None\n","20:57:17 |     dropout: 0.2\n","20:57:17 |     dynamic_batching: None\n","20:57:17 |     embedding_projection: random\n","20:57:17 |     embedding_type: random\n","20:57:17 |     embeddingsize: 128\n","20:57:17 |     encoder: lstm\n","20:57:17 |     eval_batchsize: None\n","20:57:17 |     eval_dynamic_batching: None\n","20:57:17 |     evaltask: None\n","20:57:17 |     final_extra_opt: \n","20:57:17 |     force_fp16_tokens: False\n","20:57:17 |     fp16: False\n","20:57:17 |     fp16_impl: safe\n","20:57:17 |     gpu: -1\n","20:57:17 |     gradient_clip: 0.1\n","20:57:17 |     hiddensize: 1024\n","20:57:17 |     hide_labels: False\n","20:57:17 |     history_add_global_end_token: None\n","20:57:17 |     history_reversed: False\n","20:57:17 |     history_size: -1\n","20:57:17 |     image_cropsize: 224\n","20:57:17 |     image_mode: raw\n","20:57:17 |     image_size: 256\n","20:57:17 |     inference: greedy\n","20:57:17 |     init_model: None\n","20:57:17 |     init_opt: None\n","20:57:17 |     input_dropout: 0.0\n","20:57:17 |     interactive_mode: False\n","20:57:17 |     invsqrt_lr_decay_gamma: -1\n","20:57:17 |     is_debug: False\n","20:57:17 |     label_truncate: None\n","20:57:17 |     learningrate: 1\n","20:57:17 |     load_from_checkpoint: True\n","20:57:17 |     log_every_n_secs: 10\n","20:57:17 |     log_every_n_steps: 50\n","20:57:17 |     loglevel: info\n","20:57:17 |     lookuptable: unique\n","20:57:17 |     lr_scheduler: reduceonplateau\n","20:57:17 |     lr_scheduler_decay: 0.5\n","20:57:17 |     lr_scheduler_patience: 3\n","20:57:17 |     max_train_steps: -1\n","20:57:17 |     max_train_time: -1\n","20:57:17 |     metrics: default\n","20:57:17 |     model: parlai.agents.seq2seq.seq2seq:Seq2seqAgent\n","20:57:17 |     model_file: /tmp/myseq2seqmodel\n","20:57:17 |     momentum: 0\n","20:57:17 |     multitask_weights: [1]\n","20:57:17 |     mutators: None\n","20:57:17 |     nesterov: True\n","20:57:17 |     no_cuda: False\n","20:57:17 |     num_epochs: 0.01\n","20:57:17 |     num_workers: 0\n","20:57:17 |     numlayers: 1\n","20:57:17 |     numsoftmax: 1\n","20:57:17 |     nus: (0.7,)\n","20:57:17 |     optimizer: sgd\n","20:57:17 |     override: \"{'num_epochs': 0.01, 'task': 'personachat'}\"\n","20:57:17 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n","20:57:17 |     person_tokens: False\n","20:57:17 |     personachat_attnsentlevel: True\n","20:57:17 |     personachat_learnreweight: True\n","20:57:17 |     personachat_reweight: use\n","20:57:17 |     personachat_sharelt: True\n","20:57:17 |     rank_candidates: True\n","20:57:17 |     rnn_class: lstm\n","20:57:17 |     save_after_valid: False\n","20:57:17 |     save_every_n_secs: 180\n","20:57:17 |     short_final_eval: False\n","20:57:17 |     skip_generation: False\n","20:57:17 |     special_tok_lst: None\n","20:57:17 |     split_lines: False\n","20:57:17 |     starttime: Mar27_20-56\n","20:57:17 |     task: personachat\n","20:57:17 |     temperature: 1.0\n","20:57:17 |     tensorboard_log: False\n","20:57:17 |     tensorboard_logdir: None\n","20:57:17 |     text_truncate: None\n","20:57:17 |     topk: 10\n","20:57:17 |     topp: 0.9\n","20:57:17 |     truncate: 100\n","20:57:17 |     update_freq: 1\n","20:57:17 |     use_reply: label\n","20:57:17 |     validation_cutoff: 1.0\n","20:57:17 |     validation_every_n_epochs: -1\n","20:57:17 |     validation_every_n_secs: 100\n","20:57:17 |     validation_every_n_steps: -1\n","20:57:17 |     validation_max_exs: -1\n","20:57:17 |     validation_metric: ppl\n","20:57:17 |     validation_metric_mode: max\n","20:57:17 |     validation_patience: 10\n","20:57:17 |     validation_share_agent: False\n","20:57:17 |     verbose: False\n","20:57:17 |     wandb_entity: None\n","20:57:17 |     wandb_log: False\n","20:57:17 |     wandb_name: None\n","20:57:17 |     wandb_project: None\n","20:57:17 |     warmup_rate: 0.0001\n","20:57:17 |     warmup_updates: -1\n","20:57:17 |     weight_decay: None\n","20:57:17 | Current internal commit: c4b46ccfdc0e6a03e35aefea8fc79f767c86e4ba\n","20:57:17 | Current fb commit: c4b46ccfdc0e6a03e35aefea8fc79f767c86e4ba\n","20:57:17 | creating task(s): personachat\n","20:57:17 | loading fbdialog data: /usr/local/lib/python3.7/dist-packages/data/Persona-Chat/personachat/train_self_original.txt\n","20:57:19 | training...\n","20:57:23 | time:4s total_exs:768 total_steps:6 epochs:0.01 time_left:0s\n","    clen  \\\n","   115.2   \n","    clip  \\\n","       1   \n","    ctpb  \\\n","   11238   \n","    ctps  \\\n","   16422   \n","    ctrunc  \\\n","     .5729   \n","    ctrunclen  \\\n","        27.37   \n","    exps  \\\n","   186.5   \n","    exs  \\\n","    768   \n","    gnorm  \\\n","     1.66   \n","    gpu_mem  \\\n","      .1382   \n","    llen  \\\n","   13.31   \n","    loss  \\\n","   9.535   \n","    lr  \\\n","     1   \n","    ltpb  \\\n","    1704   \n","    ltps  \\\n","    2489   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   13836   \n","    token_acc  \\\n","       .05107   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                      6   \n","     tpb  \\\n","   12941   \n","     tps  \\\n","   18911   \n","     ups  \n","   1.463\n","\n","20:57:23 | num_epochs completed:0.01 time elapsed:4.210179328918457s\n","20:57:23 | Using CUDA\n","20:57:23 | loading dictionary from /tmp/myseq2seqmodel.dict\n","20:57:23 | num words = 19244\n","20:57:24 | Total parameters: 20,139,436 (20,139,436 trainable)\n","20:57:24 | Loading existing model params from /tmp/myseq2seqmodel\n","20:57:24 | creating task(s): personachat\n","20:57:24 | loading fbdialog data: /usr/local/lib/python3.7/dist-packages/data/Persona-Chat/personachat/valid_self_original.txt\n","20:57:24 | running eval: valid\n","21:02:00 | eval completed in 275.85s\n","21:02:00 | \u001b[1mvalid:\n","    accuracy  \\\n","           0   \n","      bleu-4  \\\n","   1.023e-12   \n","    clen  \\\n","   141.8   \n","    ctpb  \\\n","   11195   \n","    ctps  \\\n","    2598   \n","    ctrunc  \\\n","     .7059   \n","    ctrunclen  \\\n","        49.99   \n","    exps  \\\n","   28.29   \n","    exs  \\\n","   7801   \n","      f1  \\\n","   .1163   \n","    gpu_mem  \\\n","     .00683   \n","    hits@1  \\\n","    .04961   \n","    hits@10  \\\n","      .5074   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .2516   \n","    llen  \\\n","    13.1   \n","    loss  \\\n","   9.042   \n","    lr  \\\n","     1   \n","    ltpb  \\\n","    1597   \n","    ltps  \\\n","   370.7   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","    ppl  \\\n","   8449   \n","    token_acc  \\\n","        .1362   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                      6   \n","     tpb  \\\n","   12792   \n","    tps  \n","   2969\n","\u001b[0m\n","21:02:00 | creating task(s): personachat\n","21:02:00 | loading fbdialog data: /usr/local/lib/python3.7/dist-packages/data/Persona-Chat/personachat/test_self_original.txt\n","21:02:00 | running eval: test\n","21:06:23 | eval completed in 262.63s\n","21:06:23 | \u001b[1mtest:\n","    accuracy  \\\n","           0   \n","      bleu-4  \\\n","   1.099e-12   \n","    clen  \\\n","   138.3   \n","    ctpb  \\\n","   10873   \n","    ctps  \\\n","    2609   \n","    ctrunc  \\\n","     .6917   \n","    ctrunclen  \\\n","        47.09   \n","    exps  \\\n","   28.61   \n","    exs  \\\n","   7512   \n","      f1  \\\n","   .1201   \n","    gpu_mem  \\\n","    .006796   \n","    hits@1  \\\n","    .05498   \n","    hits@10  \\\n","      .5097   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .2618   \n","    llen  \\\n","   12.96   \n","    loss  \\\n","   9.027   \n","    lr  \\\n","     1   \n","    ltpb  \\\n","    1546   \n","    ltps  \\\n","   370.9   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","    ppl  \\\n","   8321   \n","    token_acc  \\\n","        .1371   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                      6   \n","     tpb  \\\n","   12418   \n","    tps  \n","   2980\n","\u001b[0m\n"]}],"source":["!python train.py -eps 0.01 -t personachat\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3842,"status":"ok","timestamp":1648433471001,"user":{"displayName":"Om Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05317090667729904210"},"user_tz":240},"id":"aBgEXthc6IXf","outputId":"4a619909-73f2-4d58-8a1d-0719a0db894e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"]}],"source":["!pip install nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4339,"status":"ok","timestamp":1648433475322,"user":{"displayName":"Om Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05317090667729904210"},"user_tz":240},"id":"jlAhfTFG6qeI","outputId":"04440c3e-1a8d-4149-e43d-e020a9ffb960"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: py-rouge in /usr/local/lib/python3.7/dist-packages (1.1)\n"]}],"source":["!pip install py-rouge"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":317,"status":"ok","timestamp":1648433502715,"user":{"displayName":"Om Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05317090667729904210"},"user_tz":240},"id":"qQv_bjW67CLR","outputId":"7dcbbed9-7c76-46d0-d2fe-f90a8ff84a3b"},"outputs":[{"output_type":"stream","name":"stdout","text":["/\n"]}],"source":["%cd .."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1648433475614,"user":{"displayName":"Om Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05317090667729904210"},"user_tz":240},"id":"C161S8RK7Aws","outputId":"5262c891-6160-44d5-dde1-01090842763d"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: '/content/ParlAI'\n","/content\n"]}],"source":["%cd /content/ParlAI"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1714,"status":"ok","timestamp":1648433478750,"user":{"displayName":"Om Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05317090667729904210"},"user_tz":240},"id":"hNG9p8okDO_O","outputId":"620fb29c-8275-412b-eb46-6f775ae48e09"},"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}],"source":["import nltk\n","nltk.download('punkt')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13199,"status":"ok","timestamp":1648433492642,"user":{"displayName":"Om Kumar","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05317090667729904210"},"user_tz":240},"id":"GLWwlhqFNH1r","outputId":"2cadc2de-7a39-4c8a-f38c-8d2e982bcbd4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting bert-score\n","  Downloading bert_score-0.3.11-py3-none-any.whl (60 kB)\n","\u001b[?25l\r\u001b[K     |█████▌                          | 10 kB 20.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 20 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 30 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 40 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 51 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 60 kB 2.8 MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from bert-score) (3.2.2)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from bert-score) (1.3.5)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from bert-score) (1.10.0+cu111)\n","Collecting transformers>=3.0.0numpy\n","  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n","\u001b[K     |████████████████████████████████| 3.8 MB 8.3 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from bert-score) (21.3)\n","Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.7/dist-packages (from bert-score) (4.62.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bert-score) (2.27.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->bert-score) (3.0.7)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->bert-score) (1.21.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->bert-score) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0.1->bert-score) (1.15.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->bert-score) (3.10.0.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0numpy->bert-score) (4.2.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0numpy->bert-score) (0.4.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0numpy->bert-score) (6.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 36.1 MB/s \n","\u001b[?25hRequirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0numpy->bert-score) (0.11.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0numpy->bert-score) (2022.3.15)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=3.0.0numpy->bert-score) (3.6.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=3.0.0numpy->bert-score) (3.7.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert-score) (1.4.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->bert-score) (0.11.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bert-score) (2.10)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->bert-score) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bert-score) (2021.10.8)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bert-score) (1.26.9)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.0.0numpy->bert-score) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=3.0.0numpy->bert-score) (7.1.2)\n","Installing collected packages: sacremoses, transformers, bert-score\n","Successfully installed bert-score-0.3.11 sacremoses-0.0.49 transformers-4.17.0\n"]}],"source":["!pip install bert-score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xobOyP9P_1yj","outputId":"0df26eb1-51e9-440f-9866-a2ab95b41694"},"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n","  warnings.warn(\"The twython library has not been installed. \"\n","02:16:52 | \u001b[33mOverriding opt[\"model_file\"] to content/seq2seqmodel (previously: /tmp/seq2seqmodel)\u001b[0m\n","02:16:52 | \u001b[33mOverriding opt[\"task\"] to personachat (previously: personachat:both_revised)\u001b[0m\n","02:16:52 | \u001b[33mOverriding opt[\"metrics\"] to rouge (previously: default)\u001b[0m\n","02:16:52 | Using CUDA\n","02:16:52 | loading dictionary from content/seq2seqmodel.dict\n","02:16:52 | num words = 19615\n","02:16:55 | Total parameters: 20,282,271 (20,282,271 trainable)\n","02:16:55 | Loading existing model params from content/seq2seqmodel\n","02:16:55 | Opt:\n","02:16:55 |     adafactor_eps: '[1e-30, 0.001]'\n","02:16:55 |     adam_eps: 1e-08\n","02:16:55 |     add_p1_after_newln: False\n","02:16:55 |     aggregate_micro: False\n","02:16:55 |     allow_missing_init_opts: False\n","02:16:55 |     area_under_curve_class: None\n","02:16:55 |     area_under_curve_digits: -1\n","02:16:55 |     attention: general\n","02:16:55 |     attention_length: 48\n","02:16:55 |     attention_time: post\n","02:16:55 |     batchsize: 256\n","02:16:55 |     beam_block_full_context: True\n","02:16:55 |     beam_block_list_filename: None\n","02:16:55 |     beam_block_ngram: -1\n","02:16:55 |     beam_context_block_ngram: -1\n","02:16:55 |     beam_delay: 30\n","02:16:55 |     beam_length_penalty: 0.65\n","02:16:55 |     beam_min_length: 1\n","02:16:55 |     beam_size: 1\n","02:16:55 |     betas: '[0.9, 0.999]'\n","02:16:55 |     bidirectional: False\n","02:16:55 |     bpe_add_prefix_space: None\n","02:16:55 |     bpe_debug: False\n","02:16:55 |     bpe_dropout: None\n","02:16:55 |     bpe_merge: None\n","02:16:55 |     bpe_vocab: None\n","02:16:55 |     compute_tokenized_bleu: False\n","02:16:55 |     datapath: /usr/local/lib/python3.7/dist-packages/data\n","02:16:55 |     datatype: train\n","02:16:55 |     decoder: same\n","02:16:55 |     delimiter: '\\n'\n","02:16:55 |     dict_class: parlai.core.dict:DictionaryAgent\n","02:16:55 |     dict_endtoken: __end__\n","02:16:55 |     dict_file: content/seq2seqmodel.dict\n","02:16:55 |     dict_include_test: False\n","02:16:55 |     dict_include_valid: True\n","02:16:55 |     dict_initpath: None\n","02:16:55 |     dict_language: english\n","02:16:55 |     dict_loaded: True\n","02:16:55 |     dict_lower: True\n","02:16:55 |     dict_max_ngram_size: -1\n","02:16:55 |     dict_maxexs: -1\n","02:16:55 |     dict_maxtokens: -1\n","02:16:55 |     dict_minfreq: 0\n","02:16:55 |     dict_nulltoken: __null__\n","02:16:55 |     dict_starttoken: __start__\n","02:16:55 |     dict_textfields: text,labels\n","02:16:55 |     dict_tokenizer: split\n","02:16:55 |     dict_unktoken: __unk__\n","02:16:55 |     display_examples: False\n","02:16:55 |     download_path: None\n","02:16:55 |     dropout: 0.5\n","02:16:55 |     dynamic_batching: None\n","02:16:55 |     embedding_projection: random\n","02:16:55 |     embedding_type: random\n","02:16:55 |     embeddingsize: 128\n","02:16:55 |     encoder: lstm\n","02:16:55 |     eval_batchsize: None\n","02:16:55 |     eval_dynamic_batching: None\n","02:16:55 |     evaltask: None\n","02:16:55 |     final_extra_opt: \n","02:16:55 |     force_fp16_tokens: False\n","02:16:55 |     fp16: False\n","02:16:55 |     fp16_impl: safe\n","02:16:55 |     gpu: -1\n","02:16:55 |     gradient_clip: 0.1\n","02:16:55 |     hiddensize: 1024\n","02:16:55 |     hide_labels: False\n","02:16:55 |     history_add_global_end_token: None\n","02:16:55 |     history_reversed: False\n","02:16:55 |     history_size: -1\n","02:16:55 |     image_cropsize: 224\n","02:16:55 |     image_mode: raw\n","02:16:55 |     image_size: 256\n","02:16:55 |     inference: greedy\n","02:16:55 |     init_model: None\n","02:16:55 |     init_opt: None\n","02:16:55 |     input_dropout: 0.0\n","02:16:55 |     interactive_mode: False\n","02:16:55 |     invsqrt_lr_decay_gamma: -1\n","02:16:55 |     is_debug: False\n","02:16:55 |     label_truncate: None\n","02:16:55 |     learningrate: 1.2\n","02:16:55 |     log_every_n_secs: 30\n","02:16:55 |     log_every_n_steps: 50\n","02:16:55 |     log_keep_fields: all\n","02:16:55 |     loglevel: info\n","02:16:55 |     lookuptable: unique\n","02:16:55 |     lr_scheduler: reduceonplateau\n","02:16:55 |     lr_scheduler_decay: 0.5\n","02:16:55 |     lr_scheduler_patience: 3\n","02:16:55 |     max_train_steps: -1\n","02:16:55 |     max_train_time: -1\n","02:16:55 |     metrics: rouge\n","02:16:55 |     model: parlai.agents.seq2seq.seq2seq:Seq2seqAgent\n","02:16:55 |     model_file: content/seq2seqmodel\n","02:16:55 |     momentum: 0\n","02:16:55 |     multitask_weights: [1]\n","02:16:55 |     mutators: None\n","02:16:55 |     nesterov: True\n","02:16:55 |     no_cuda: False\n","02:16:55 |     num_epochs: 20.0\n","02:16:55 |     num_examples: -1\n","02:16:55 |     num_workers: 0\n","02:16:55 |     numlayers: 1\n","02:16:55 |     numsoftmax: 1\n","02:16:55 |     nus: [0.7]\n","02:16:55 |     optimizer: sgd\n","02:16:55 |     override: \"{'model_file': 'content/seq2seqmodel', 'task': 'personachat', 'metrics': 'rouge'}\"\n","02:16:55 |     parlai_home: /usr/local/lib/python3.7/dist-packages\n","02:16:55 |     person_tokens: False\n","02:16:55 |     personachat_attnsentlevel: True\n","02:16:55 |     personachat_learnreweight: True\n","02:16:55 |     personachat_reweight: use\n","02:16:55 |     personachat_sharelt: True\n","02:16:55 |     rank_candidates: True\n","02:16:55 |     report_filename: \n","02:16:55 |     rnn_class: lstm\n","02:16:55 |     save_after_valid: False\n","02:16:55 |     save_every_n_secs: 300\n","02:16:55 |     save_format: conversations\n","02:16:55 |     short_final_eval: False\n","02:16:55 |     skip_generation: False\n","02:16:55 |     special_tok_lst: None\n","02:16:55 |     split_lines: False\n","02:16:55 |     starttime: Mar27_13-59\n","02:16:55 |     task: personachat\n","02:16:55 |     temperature: 1.0\n","02:16:55 |     tensorboard_log: False\n","02:16:55 |     tensorboard_logdir: None\n","02:16:55 |     text_truncate: None\n","02:16:55 |     topk: 10\n","02:16:55 |     topp: 0.9\n","02:16:55 |     truncate: 100\n","02:16:55 |     update_freq: 1\n","02:16:55 |     use_reply: label\n","02:16:55 |     validation_cutoff: 1.0\n","02:16:55 |     validation_every_n_epochs: -1\n","02:16:55 |     validation_every_n_secs: 2000\n","02:16:55 |     validation_every_n_steps: -1\n","02:16:55 |     validation_max_exs: -1\n","02:16:55 |     validation_metric: ppl\n","02:16:55 |     validation_metric_mode: max\n","02:16:55 |     validation_patience: 10\n","02:16:55 |     validation_share_agent: False\n","02:16:55 |     verbose: False\n","02:16:55 |     wandb_entity: None\n","02:16:55 |     wandb_log: False\n","02:16:55 |     wandb_name: None\n","02:16:55 |     wandb_project: None\n","02:16:55 |     warmup_rate: 0.0001\n","02:16:55 |     warmup_updates: -1\n","02:16:55 |     weight_decay: None\n","02:16:55 |     world_logs: \n","02:16:55 | Evaluating task personachat using datatype valid.\n","02:16:55 | creating task(s): personachat\n","02:16:55 | loading fbdialog data: /usr/local/lib/python3.7/dist-packages/data/Persona-Chat/personachat/valid_self_original.txt\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8752)\n","02:17:08 | 0.0% complete (1 / 7,801), 0:00:12 elapsed, 1 day, 3:47:44 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","      49   \n","    ctpb  \\\n","      49   \n","    ctps  \\\n","     509   \n","    ctrunc  \\\n","         0   \n","    ctrunclen  \\\n","            0   \n","    exps  \\\n","   10.36   \n","    exs  \\\n","      1   \n","      f1  \\\n","   .2857   \n","    gpu_mem  \\\n","     .09603   \n","    hits@1  \\\n","         0   \n","    hits@10  \\\n","          1   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","         1   \n","    llen  \\\n","      18   \n","    loss  \\\n","   4.059   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","      18   \n","    ltps  \\\n","   186.9   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   57.94   \n","    rouge_1  \\\n","      .2143   \n","    rouge_2  \\\n","     .07692   \n","    rouge_L  \\\n","      .2143   \n","    token_acc  \\\n","        .2778   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","    tpb  \\\n","     67   \n","    tps  \n","    696\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8054)\n","02:17:20 | 0.0% complete (2 / 7,801), 0:00:25 elapsed, 1 day, 3:14:53 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","    61.5   \n","    ctpb  \\\n","    61.5   \n","    ctps  \\\n","   9.477   \n","    ctrunc  \\\n","         0   \n","    ctrunclen  \\\n","            0   \n","    exps  \\\n","   .1541   \n","    exs  \\\n","      2   \n","      f1  \\\n","   .2229   \n","    gpu_mem  \\\n","     .09606   \n","    hits@1  \\\n","         0   \n","    hits@10  \\\n","      .5000   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .5000   \n","    llen  \\\n","      16   \n","    loss  \\\n","   5.058   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","      16   \n","    ltps  \\\n","   2.466   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   157.2   \n","    rouge_1  \\\n","      .1981   \n","    rouge_2  \\\n","     .03846   \n","    rouge_L  \\\n","      .1981   \n","    token_acc  \\\n","        .2812   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","    tpb  \\\n","   77.5   \n","     tps  \n","   11.94\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8182)\n","02:17:33 | 0.0% complete (3 / 7,801), 0:00:37 elapsed, 1 day, 2:53:59 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   74.67   \n","    ctpb  \\\n","   74.33   \n","    ctps  \\\n","    8.81   \n","    ctrunc  \\\n","     .3333   \n","    ctrunclen  \\\n","        .3333   \n","    exps  \\\n","   .1185   \n","    exs  \\\n","      3   \n","      f1  \\\n","   .1486   \n","    gpu_mem  \\\n","     .09606   \n","    hits@1  \\\n","         0   \n","    hits@10  \\\n","      .6667   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .3333   \n","    llen  \\\n","   13.67   \n","    loss  \\\n","   5.157   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   13.67   \n","    ltps  \\\n","    1.62   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   173.7   \n","    rouge_1  \\\n","      .1320   \n","    rouge_2  \\\n","     .02564   \n","    rouge_L  \\\n","      .1320   \n","    token_acc  \\\n","        .2683   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","    tpb  \\\n","     88   \n","     tps  \n","   10.43\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8192)\n","02:17:45 | 0.1% complete (4 / 7,801), 0:00:49 elapsed, 1 day, 2:41:35 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","    87.5   \n","    ctpb  \\\n","   80.75   \n","    ctps  \\\n","   8.633   \n","    ctrunc  \\\n","     .5000   \n","    ctrunclen  \\\n","         6.75   \n","    exps  \\\n","   .1069   \n","    exs  \\\n","      4   \n","      f1  \\\n","   .1364   \n","    gpu_mem  \\\n","     .09606   \n","    hits@1  \\\n","         0   \n","    hits@10  \\\n","      .7500   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .5000   \n","    llen  \\\n","    12.5   \n","    loss  \\\n","   4.939   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","    12.5   \n","    ltps  \\\n","   1.336   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   139.6   \n","    rouge_1  \\\n","      .1407   \n","    rouge_2  \\\n","     .01923   \n","    rouge_L  \\\n","      .1407   \n","    token_acc  \\\n","        .3000   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   93.25   \n","     tps  \n","   9.969\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.7989)\n","02:17:57 | 0.1% complete (5 / 7,801), 0:01:01 elapsed, 1 day, 2:36:31 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","    99.2   \n","    ctpb  \\\n","    84.6   \n","    ctps  \\\n","   8.551   \n","    ctrunc  \\\n","     .6000   \n","    ctrunclen  \\\n","         14.6   \n","    exps  \\\n","   .1011   \n","    exs  \\\n","      5   \n","      f1  \\\n","   .1229   \n","    gpu_mem  \\\n","     .09606   \n","    hits@1  \\\n","         0   \n","    hits@10  \\\n","      .8000   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4000   \n","    llen  \\\n","    13.8   \n","    loss  \\\n","   5.018   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","    13.8   \n","    ltps  \\\n","   1.395   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   151.1   \n","    rouge_1  \\\n","      .1259   \n","    rouge_2  \\\n","     .01538   \n","    rouge_L  \\\n","      .1259   \n","    token_acc  \\\n","        .2899   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","    tpb  \\\n","   98.4   \n","     tps  \n","   9.946\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8128)\n","02:18:09 | 0.1% complete (6 / 7,801), 0:01:13 elapsed, 1 day, 2:31:24 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   112.7   \n","    ctpb  \\\n","   87.17   \n","    ctps  \\\n","   8.492   \n","    ctrunc  \\\n","     .6667   \n","    ctrunclen  \\\n","         25.5   \n","     exps  \\\n","   .09742   \n","    exs  \\\n","      6   \n","      f1  \\\n","   .1271   \n","    gpu_mem  \\\n","     .09606   \n","    hits@1  \\\n","         0   \n","    hits@10  \\\n","      .6667   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .3333   \n","    llen  \\\n","   14.17   \n","    loss  \\\n","   5.174   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   14.17   \n","    ltps  \\\n","    1.38   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   176.6   \n","    rouge_1  \\\n","      .1305   \n","    rouge_2  \\\n","     .01282   \n","    rouge_L  \\\n","      .1305   \n","    token_acc  \\\n","        .2941   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   101.3   \n","     tps  \n","   9.872\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8234)\n","02:18:21 | 0.1% complete (7 / 7,801), 0:01:25 elapsed, 1 day, 2:29:06 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   125.6   \n","    ctpb  \\\n","      89   \n","    ctps  \\\n","   8.458   \n","    ctrunc  \\\n","     .7143   \n","    ctrunclen  \\\n","        36.57   \n","     exps  \\\n","   .09503   \n","    exs  \\\n","      7   \n","      f1  \\\n","   .1338   \n","    gpu_mem  \\\n","     .09606   \n","    hits@1  \\\n","         0   \n","    hits@10  \\\n","      .5714   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .2857   \n","    llen  \\\n","   13.86   \n","    loss  \\\n","   5.248   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   13.86   \n","    ltps  \\\n","   1.317   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   190.3   \n","    rouge_1  \\\n","      .1436   \n","    rouge_2  \\\n","     .02885   \n","    rouge_L  \\\n","      .1436   \n","    token_acc  \\\n","        .2990   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   102.9   \n","     tps  \n","   9.774\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.7916)\n","02:18:33 | 0.1% complete (8 / 7,801), 0:01:37 elapsed, 1 day, 2:27:24 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   137.8   \n","    ctpb  \\\n","   90.38   \n","    ctps  \\\n","   8.427   \n","    ctrunc  \\\n","     .7500   \n","    ctrunclen  \\\n","        47.38   \n","     exps  \\\n","   .09324   \n","    exs  \\\n","      8   \n","      f1  \\\n","   .1349   \n","    gpu_mem  \\\n","     .09608   \n","    hits@1  \\\n","         0   \n","    hits@10  \\\n","      .5000   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .2500   \n","    llen  \\\n","   13.88   \n","    loss  \\\n","   5.402   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   13.88   \n","    ltps  \\\n","   1.294   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","    ppl  \\\n","    222   \n","    rouge_1  \\\n","      .1465   \n","    rouge_2  \\\n","      .0366   \n","    rouge_L  \\\n","      .1465   \n","    token_acc  \\\n","        .2883   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   104.2   \n","     tps  \n","   9.721\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.9027)\n","02:18:45 | 0.1% complete (9 / 7,801), 0:01:49 elapsed, 1 day, 2:22:16 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   127.4   \n","    ctpb  \\\n","   85.33   \n","    ctps  \\\n","   7.846   \n","    ctrunc  \\\n","     .6667   \n","    ctrunclen  \\\n","        42.11   \n","     exps  \\\n","   .09194   \n","    exs  \\\n","      9   \n","      f1  \\\n","   .1592   \n","    gpu_mem  \\\n","     .09599   \n","    hits@1  \\\n","         0   \n","    hits@10  \\\n","      .5556   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .3333   \n","    llen  \\\n","   13.78   \n","    loss  \\\n","   5.175   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   13.78   \n","    ltps  \\\n","   1.267   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   176.9   \n","    rouge_1  \\\n","      .1636   \n","    rouge_2  \\\n","     .05723   \n","    rouge_L  \\\n","      .1636   \n","    token_acc  \\\n","        .3145   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   99.11   \n","     tps  \n","   9.113\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.7992)\n","02:18:57 | 0.1% complete (10 / 7,801), 0:02:01 elapsed, 1 day, 2:20:13 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   121.5   \n","    ctpb  \\\n","    83.6   \n","    ctps  \\\n","   7.614   \n","    ctrunc  \\\n","     .6000   \n","    ctrunclen  \\\n","         37.9   \n","     exps  \\\n","   .09107   \n","    exs  \\\n","     10   \n","      f1  \\\n","   .1433   \n","    gpu_mem  \\\n","     .09606   \n","    hits@1  \\\n","         0   \n","    hits@10  \\\n","      .6000   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4000   \n","    llen  \\\n","    13.8   \n","    loss  \\\n","   4.995   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","    13.8   \n","    ltps  \\\n","   1.257   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   147.6   \n","    rouge_1  \\\n","      .1472   \n","    rouge_2  \\\n","     .05151   \n","    rouge_L  \\\n","      .1472   \n","    token_acc  \\\n","        .3261   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","    tpb  \\\n","   97.4   \n","    tps  \n","   8.87\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8100)\n","02:19:09 | 0.1% complete (11 / 7,801), 0:02:13 elapsed, 1 day, 2:21:35 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   118.7   \n","    ctpb  \\\n","   84.27   \n","    ctps  \\\n","   7.607   \n","    ctrunc  \\\n","     .5455   \n","    ctrunclen  \\\n","        34.45   \n","     exps  \\\n","   .09027   \n","    exs  \\\n","     11   \n","      f1  \\\n","   .1370   \n","    gpu_mem  \\\n","     .09606   \n","    hits@1  \\\n","         0   \n","    hits@10  \\\n","      .6364   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4545   \n","    llen  \\\n","   14.09   \n","    loss  \\\n","   4.942   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   14.09   \n","    ltps  \\\n","   1.272   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","    ppl  \\\n","    140   \n","    rouge_1  \\\n","      .1408   \n","    rouge_2  \\\n","     .04682   \n","    rouge_L  \\\n","      .1408   \n","    token_acc  \\\n","        .3290   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   98.36   \n","     tps  \n","   8.879\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8312)\n","02:19:21 | 0.2% complete (12 / 7,801), 0:02:26 elapsed, 1 day, 2:19:31 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   118.8   \n","    ctpb  \\\n","   85.58   \n","    ctps  \\\n","   7.656   \n","    ctrunc  \\\n","     .5833   \n","    ctrunclen  \\\n","        33.25   \n","     exps  \\\n","   .08945   \n","    exs  \\\n","     12   \n","      f1  \\\n","   .1400   \n","    gpu_mem  \\\n","     .09606   \n","    hits@1  \\\n","         0   \n","    hits@10  \\\n","      .6667   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4167   \n","    llen  \\\n","   14.75   \n","    loss  \\\n","   5.018   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   14.75   \n","    ltps  \\\n","   1.319   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","    ppl  \\\n","    151   \n","    rouge_1  \\\n","      .1395   \n","    rouge_2  \\\n","     .04292   \n","    rouge_L  \\\n","      .1343   \n","    token_acc  \\\n","        .3220   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   100.3   \n","     tps  \n","   8.975\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8613)\n","02:19:33 | 0.2% complete (13 / 7,801), 0:02:37 elapsed, 1 day, 2:17:33 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   121.5   \n","    ctpb  \\\n","   86.69   \n","    ctps  \\\n","   7.711   \n","    ctrunc  \\\n","     .6154   \n","    ctrunclen  \\\n","        34.77   \n","     exps  \\\n","   .08895   \n","    exs  \\\n","     13   \n","      f1  \\\n","   .1512   \n","    gpu_mem  \\\n","     .09603   \n","    hits@1  \\\n","         0   \n","    hits@10  \\\n","      .6923   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .3846   \n","    llen  \\\n","      15   \n","    loss  \\\n","   5.032   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","      15   \n","    ltps  \\\n","   1.334   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   153.3   \n","    rouge_1  \\\n","      .1453   \n","    rouge_2  \\\n","     .03962   \n","    rouge_L  \\\n","      .1350   \n","    token_acc  \\\n","        .3231   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   101.7   \n","     tps  \n","   9.045\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8469)\n","02:19:46 | 0.2% complete (14 / 7,801), 0:02:50 elapsed, 1 day, 2:18:36 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   126.2   \n","    ctpb  \\\n","   87.64   \n","    ctps  \\\n","   7.759   \n","    ctrunc  \\\n","     .6429   \n","    ctrunclen  \\\n","        38.57   \n","     exps  \\\n","   .08853   \n","    exs  \\\n","     14   \n","      f1  \\\n","   .1464   \n","    gpu_mem  \\\n","     .09605   \n","    hits@1  \\\n","         0   \n","    hits@10  \\\n","      .6429   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .3571   \n","    llen  \\\n","   15.43   \n","    loss  \\\n","   5.086   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   15.43   \n","    ltps  \\\n","   1.366   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   161.7   \n","    rouge_1  \\\n","      .1393   \n","    rouge_2  \\\n","     .03679   \n","    rouge_L  \\\n","      .1298   \n","    token_acc  \\\n","        .3194   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   103.1   \n","     tps  \n","   9.124\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8406)\n","02:19:58 | 0.2% complete (15 / 7,801), 0:03:02 elapsed, 1 day, 2:18:33 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   132.4   \n","    ctpb  \\\n","   88.47   \n","    ctps  \\\n","   7.786   \n","    ctrunc  \\\n","     .6667   \n","    ctrunclen  \\\n","        43.93   \n","     exps  \\\n","   .08801   \n","    exs  \\\n","     15   \n","      f1  \\\n","   .1427   \n","    gpu_mem  \\\n","     .09604   \n","    hits@1  \\\n","         0   \n","    hits@10  \\\n","      .6000   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .3333   \n","    llen  \\\n","   15.73   \n","    loss  \\\n","   5.204   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   15.73   \n","    ltps  \\\n","   1.385   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   182.1   \n","    rouge_1  \\\n","      .1345   \n","    rouge_2  \\\n","     .03434   \n","    rouge_L  \\\n","      .1256   \n","    token_acc  \\\n","        .3178   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   104.2   \n","     tps  \n","   9.171\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8579)\n","02:20:10 | 0.2% complete (16 / 7,801), 0:03:14 elapsed, 1 day, 2:15:49 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   127.2   \n","    ctpb  \\\n","      86   \n","    ctps  \\\n","   7.537   \n","    ctrunc  \\\n","     .6250   \n","    ctrunclen  \\\n","        41.19   \n","     exps  \\\n","   .08764   \n","    exs  \\\n","     16   \n","      f1  \\\n","   .1434   \n","    gpu_mem  \\\n","     .09596   \n","    hits@1  \\\n","         0   \n","    hits@10  \\\n","      .6250   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .3750   \n","    llen  \\\n","   15.25   \n","    loss  \\\n","   5.194   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   15.25   \n","    ltps  \\\n","   1.336   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   180.2   \n","    rouge_1  \\\n","      .1350   \n","    rouge_2  \\\n","     .03219   \n","    rouge_L  \\\n","      .1267   \n","    token_acc  \\\n","        .3156   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   101.2   \n","     tps  \n","   8.873\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.7855)\n","02:20:22 | 0.2% complete (17 / 7,801), 0:03:26 elapsed, 1 day, 2:15:29 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   124.1   \n","    ctpb  \\\n","   85.35   \n","    ctps  \\\n","   7.462   \n","    ctrunc  \\\n","     .5882   \n","    ctrunclen  \\\n","        38.76   \n","     exps  \\\n","   .08742   \n","    exs  \\\n","     17   \n","      f1  \\\n","   .1350   \n","    gpu_mem  \\\n","     .09606   \n","    hits@1  \\\n","         0   \n","    hits@10  \\\n","      .6471   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4118   \n","    llen  \\\n","   14.94   \n","    loss  \\\n","   5.199   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   14.94   \n","    ltps  \\\n","   1.306   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   181.2   \n","    rouge_1  \\\n","      .1271   \n","    rouge_2  \\\n","      .0303   \n","    rouge_L  \\\n","      .1192   \n","    token_acc  \\\n","        .3189   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   100.3   \n","     tps  \n","   8.768\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8175)\n","02:20:34 | 0.2% complete (18 / 7,801), 0:03:38 elapsed, 1 day, 2:14:38 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   122.9   \n","    ctpb  \\\n","   86.17   \n","    ctps  \\\n","   7.507   \n","    ctrunc  \\\n","     .6111   \n","    ctrunclen  \\\n","        36.72   \n","     exps  \\\n","   .08712   \n","    exs  \\\n","     18   \n","      f1  \\\n","   .1323   \n","    gpu_mem  \\\n","     .09605   \n","    hits@1  \\\n","         0   \n","    hits@10  \\\n","      .6111   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .3889   \n","    llen  \\\n","   14.72   \n","    loss  \\\n","   5.277   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   14.72   \n","    ltps  \\\n","   1.283   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   195.7   \n","    rouge_1  \\\n","      .1262   \n","    rouge_2  \\\n","     .02861   \n","    rouge_L  \\\n","      .1188   \n","    token_acc  \\\n","        .3094   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   100.9   \n","    tps  \n","   8.79\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8349)\n","02:20:46 | 0.2% complete (19 / 7,801), 0:03:50 elapsed, 1 day, 2:14:05 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   123.1   \n","    ctpb  \\\n","   86.89   \n","    ctps  \\\n","   7.551   \n","    ctrunc  \\\n","     .6316   \n","    ctrunclen  \\\n","        36.16   \n","    exps  \\\n","   .0869   \n","    exs  \\\n","     19   \n","      f1  \\\n","   .1370   \n","    gpu_mem  \\\n","      .0960   \n","    hits@1  \\\n","         0   \n","    hits@10  \\\n","      .5789   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .3684   \n","    llen  \\\n","   14.68   \n","    loss  \\\n","   5.365   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   14.68   \n","    ltps  \\\n","   1.276   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   213.8   \n","    rouge_1  \\\n","      .1291   \n","    rouge_2  \\\n","     .03237   \n","    rouge_L  \\\n","      .1221   \n","    token_acc  \\\n","        .3011   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   101.6   \n","     tps  \n","   8.827\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8239)\n","02:20:58 | 0.3% complete (20 / 7,801), 0:04:02 elapsed, 1 day, 2:14:04 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   124.7   \n","    ctpb  \\\n","   87.55   \n","    ctps  \\\n","   7.589   \n","    ctrunc  \\\n","     .6500   \n","    ctrunclen  \\\n","        37.15   \n","     exps  \\\n","   .08668   \n","    exs  \\\n","     20   \n","      f1  \\\n","   .1389   \n","    gpu_mem  \\\n","     .09603   \n","    hits@1  \\\n","         0   \n","    hits@10  \\\n","      .5500   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .3500   \n","    llen  \\\n","   14.85   \n","    loss  \\\n","    5.35   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   14.85   \n","    ltps  \\\n","   1.287   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   210.6   \n","    rouge_1  \\\n","      .1289   \n","    rouge_2  \\\n","     .03409   \n","    rouge_L  \\\n","      .1222   \n","    token_acc  \\\n","        .3030   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   102.4   \n","     tps  \n","   8.876\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8266)\n","02:21:10 | 0.3% complete (21 / 7,801), 0:04:14 elapsed, 1 day, 2:13:49 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   127.7   \n","    ctpb  \\\n","   88.14   \n","    ctps  \\\n","    7.62   \n","    ctrunc  \\\n","     .6667   \n","    ctrunclen  \\\n","        39.57   \n","     exps  \\\n","   .08645   \n","    exs  \\\n","     21   \n","      f1  \\\n","   .1323   \n","    gpu_mem  \\\n","     .09603   \n","    hits@1  \\\n","         0   \n","    hits@10  \\\n","      .5238   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .3333   \n","    llen  \\\n","   14.43   \n","    loss  \\\n","   5.408   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   14.43   \n","    ltps  \\\n","   1.247   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   223.2   \n","    rouge_1  \\\n","      .1228   \n","    rouge_2  \\\n","     .03246   \n","    rouge_L  \\\n","      .1164   \n","    token_acc  \\\n","        .2970   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   102.6   \n","     tps  \n","   8.868\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8236)\n","02:21:22 | 0.3% complete (22 / 7,801), 0:04:27 elapsed, 1 day, 2:13:51 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   131.3   \n","    ctpb  \\\n","   88.68   \n","    ctps  \\\n","    7.65   \n","    ctrunc  \\\n","     .6818   \n","    ctrunclen  \\\n","        42.64   \n","     exps  \\\n","   .08626   \n","    exs  \\\n","     22   \n","      f1  \\\n","   .1431   \n","    gpu_mem  \\\n","     .09603   \n","    hits@1  \\\n","         0   \n","    hits@10  \\\n","      .5000   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .3182   \n","    llen  \\\n","   14.55   \n","    loss  \\\n","   5.432   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   14.55   \n","    ltps  \\\n","   1.255   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   228.5   \n","    rouge_1  \\\n","      .1323   \n","    rouge_2  \\\n","     .03748   \n","    rouge_L  \\\n","      .1263   \n","    token_acc  \\\n","        .2969   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   103.2   \n","     tps  \n","   8.904\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.9319)\n","02:21:34 | 0.3% complete (23 / 7,801), 0:04:39 elapsed, 1 day, 2:13:15 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   127.9   \n","    ctpb  \\\n","   87.09   \n","    ctps  \\\n","   7.497   \n","    ctrunc  \\\n","     .6522   \n","    ctrunclen  \\\n","        40.78   \n","     exps  \\\n","   .08609   \n","    exs  \\\n","     23   \n","      f1  \\\n","   .1586   \n","    gpu_mem  \\\n","     .09603   \n","    hits@1  \\\n","    .04348   \n","    hits@10  \\\n","      .5217   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .3478   \n","    llen  \\\n","   14.65   \n","    loss  \\\n","   5.265   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   14.65   \n","    ltps  \\\n","   1.261   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   193.4   \n","    rouge_1  \\\n","      .1433   \n","    rouge_2  \\\n","     .04672   \n","    rouge_L  \\\n","      .1375   \n","    token_acc  \\\n","        .3145   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   101.7   \n","     tps  \n","   8.758\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8018)\n","02:21:46 | 0.3% complete (24 / 7,801), 0:04:51 elapsed, 1 day, 2:12:40 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   125.9   \n","    ctpb  \\\n","   86.83   \n","    ctps  \\\n","   7.462   \n","    ctrunc  \\\n","     .6250   \n","    ctrunclen  \\\n","        39.08   \n","     exps  \\\n","   .08594   \n","    exs  \\\n","     24   \n","      f1  \\\n","   .1596   \n","    gpu_mem  \\\n","     .09606   \n","    hits@1  \\\n","    .04167   \n","    hits@10  \\\n","      .5417   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .3750   \n","    llen  \\\n","   14.46   \n","    loss  \\\n","   5.226   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   14.46   \n","    ltps  \\\n","   1.243   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","    ppl  \\\n","    186   \n","    rouge_1  \\\n","      .1478   \n","    rouge_2  \\\n","     .05073   \n","    rouge_L  \\\n","      .1422   \n","    token_acc  \\\n","        .3199   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   101.3   \n","     tps  \n","   8.705\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.7983)\n","02:21:58 | 0.3% complete (25 / 7,801), 0:05:03 elapsed, 1 day, 2:11:52 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","     125   \n","    ctpb  \\\n","   87.36   \n","    ctps  \\\n","   7.496   \n","    ctrunc  \\\n","     .6400   \n","    ctrunclen  \\\n","        37.68   \n","     exps  \\\n","   .08581   \n","    exs  \\\n","     25   \n","      f1  \\\n","   .1563   \n","    gpu_mem  \\\n","     .09606   \n","    hits@1  \\\n","     .0400   \n","    hits@10  \\\n","      .5600   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4000   \n","    llen  \\\n","   14.48   \n","    loss  \\\n","   5.147   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   14.48   \n","    ltps  \\\n","   1.242   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   171.9   \n","    rouge_1  \\\n","      .1452   \n","    rouge_2  \\\n","      .0487   \n","    rouge_L  \\\n","      .1398   \n","    token_acc  \\\n","        .3232   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   101.8   \n","     tps  \n","   8.739\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.7942)\n","02:22:10 | 0.3% complete (26 / 7,801), 0:05:15 elapsed, 1 day, 2:11:06 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   125.2   \n","    ctpb  \\\n","   87.85   \n","    ctps  \\\n","   7.529   \n","    ctrunc  \\\n","     .6538   \n","    ctrunclen  \\\n","        37.38   \n","    exps  \\\n","   .0857   \n","    exs  \\\n","     26   \n","      f1  \\\n","   .1503   \n","    gpu_mem  \\\n","     .09606   \n","    hits@1  \\\n","    .07692   \n","    hits@10  \\\n","      .5769   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4231   \n","    llen  \\\n","   14.27   \n","    loss  \\\n","   5.083   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   14.27   \n","    ltps  \\\n","   1.223   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   161.2   \n","    rouge_1  \\\n","      .1396   \n","    rouge_2  \\\n","     .04682   \n","    rouge_L  \\\n","      .1344   \n","    token_acc  \\\n","        .3261   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   102.1   \n","     tps  \n","   8.752\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8203)\n","02:22:23 | 0.3% complete (27 / 7,801), 0:05:27 elapsed, 1 day, 2:10:47 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   126.1   \n","    ctpb  \\\n","    88.3   \n","    ctps  \\\n","   7.559   \n","    ctrunc  \\\n","     .6667   \n","    ctrunclen  \\\n","        37.81   \n","     exps  \\\n","   .08561   \n","    exs  \\\n","     27   \n","      f1  \\\n","   .1514   \n","    gpu_mem  \\\n","     .09603   \n","    hits@1  \\\n","    .07407   \n","    hits@10  \\\n","      .5926   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4444   \n","    llen  \\\n","   14.26   \n","    loss  \\\n","   5.057   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   14.26   \n","    ltps  \\\n","   1.221   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   157.1   \n","    rouge_1  \\\n","      .1418   \n","    rouge_2  \\\n","     .04509   \n","    rouge_L  \\\n","      .1369   \n","    token_acc  \\\n","        .3299   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   102.6   \n","    tps  \n","   8.78\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8424)\n","02:22:36 | 0.4% complete (28 / 7,801), 0:05:40 elapsed, 1 day, 2:14:13 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   127.9   \n","    ctpb  \\\n","   88.71   \n","    ctps  \\\n","   7.585   \n","    ctrunc  \\\n","     .6786   \n","    ctrunclen  \\\n","        39.18   \n","    exps  \\\n","   .0855   \n","    exs  \\\n","     28   \n","      f1  \\\n","   .1579   \n","    gpu_mem  \\\n","      .0960   \n","    hits@1  \\\n","    .07143   \n","    hits@10  \\\n","      .6071   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4286   \n","    llen  \\\n","   14.25   \n","    loss  \\\n","   5.071   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   14.25   \n","    ltps  \\\n","   1.218   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   159.3   \n","    rouge_1  \\\n","      .1465   \n","    rouge_2  \\\n","     .04705   \n","    rouge_L  \\\n","      .1417   \n","    token_acc  \\\n","        .3308   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","    tpb  \\\n","    103   \n","     tps  \n","   8.804\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8405)\n","02:22:48 | 0.4% complete (29 / 7,801), 0:05:52 elapsed, 1 day, 2:14:03 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   130.4   \n","    ctpb  \\\n","    89.1   \n","    ctps  \\\n","   7.592   \n","    ctrunc  \\\n","     .6897   \n","    ctrunclen  \\\n","        41.34   \n","    exps  \\\n","   .0852   \n","    exs  \\\n","     29   \n","      f1  \\\n","   .1571   \n","    gpu_mem  \\\n","     .09597   \n","    hits@1  \\\n","    .06897   \n","    hits@10  \\\n","      .5862   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4138   \n","    llen  \\\n","   14.14   \n","    loss  \\\n","   5.106   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   14.14   \n","    ltps  \\\n","   1.205   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","    ppl  \\\n","    165   \n","    rouge_1  \\\n","      .1458   \n","    rouge_2  \\\n","     .04543   \n","    rouge_L  \\\n","      .1412   \n","    token_acc  \\\n","        .3317   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   103.2   \n","     tps  \n","   8.796\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8148)\n","02:23:00 | 0.4% complete (30 / 7,801), 0:06:04 elapsed, 1 day, 2:13:57 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   127.6   \n","    ctpb  \\\n","    87.6   \n","    ctps  \\\n","   7.455   \n","    ctrunc  \\\n","     .6667   \n","    ctrunclen  \\\n","        39.97   \n","    exps  \\\n","   .0851   \n","    exs  \\\n","     30   \n","      f1  \\\n","   .1547   \n","    gpu_mem  \\\n","     .09606   \n","    hits@1  \\\n","     .1000   \n","    hits@10  \\\n","      .6000   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4333   \n","    llen  \\\n","   14.17   \n","    loss  \\\n","   5.057   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   14.17   \n","    ltps  \\\n","   1.206   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   157.1   \n","    rouge_1  \\\n","      .1446   \n","    rouge_2  \\\n","     .04391   \n","    rouge_L  \\\n","      .1402   \n","    token_acc  \\\n","        .3341   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   101.8   \n","    tps  \n","   8.66\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8379)\n","02:23:12 | 0.4% complete (31 / 7,801), 0:06:16 elapsed, 1 day, 2:14:09 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   125.8   \n","    ctpb  \\\n","   87.16   \n","    ctps  \\\n","   7.408   \n","    ctrunc  \\\n","     .6452   \n","    ctrunclen  \\\n","        38.68   \n","    exps  \\\n","   .0850   \n","    exs  \\\n","     31   \n","      f1  \\\n","   .1601   \n","    gpu_mem  \\\n","     .09606   \n","    hits@1  \\\n","    .09677   \n","    hits@10  \\\n","      .6129   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4516   \n","    llen  \\\n","   14.26   \n","    loss  \\\n","   5.034   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   14.26   \n","    ltps  \\\n","   1.212   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   153.5   \n","    rouge_1  \\\n","      .1517   \n","    rouge_2  \\\n","     .04572   \n","    rouge_L  \\\n","      .1474   \n","    token_acc  \\\n","        .3394   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   101.4   \n","    tps  \n","   8.62\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8068)\n","02:23:24 | 0.4% complete (32 / 7,801), 0:06:29 elapsed, 1 day, 2:14:47 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","     125   \n","    ctpb  \\\n","   87.53   \n","    ctps  \\\n","    7.43   \n","    ctrunc  \\\n","     .6250   \n","    ctrunclen  \\\n","        37.47   \n","     exps  \\\n","   .08488   \n","    exs  \\\n","     32   \n","      f1  \\\n","   .1577   \n","    gpu_mem  \\\n","     .09606   \n","    hits@1  \\\n","    .09375   \n","    hits@10  \\\n","      .6250   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4688   \n","    llen  \\\n","   14.28   \n","    loss  \\\n","   5.007   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   14.28   \n","    ltps  \\\n","   1.212   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   149.4   \n","    rouge_1  \\\n","      .1501   \n","    rouge_2  \\\n","     .04429   \n","    rouge_L  \\\n","      .1459   \n","    token_acc  \\\n","        .3435   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   101.8   \n","     tps  \n","   8.642\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8455)\n","02:23:37 | 0.4% complete (33 / 7,801), 0:06:41 elapsed, 1 day, 2:14:53 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","     125   \n","    ctpb  \\\n","   87.91   \n","    ctps  \\\n","   7.451   \n","    ctrunc  \\\n","     .6364   \n","    ctrunclen  \\\n","        37.06   \n","     exps  \\\n","   .08476   \n","    exs  \\\n","     33   \n","      f1  \\\n","   .1615   \n","    gpu_mem  \\\n","     .09597   \n","    hits@1  \\\n","    .09091   \n","    hits@10  \\\n","      .6364   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4848   \n","    llen  \\\n","   14.15   \n","    loss  \\\n","   4.984   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   14.15   \n","    ltps  \\\n","     1.2   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","    ppl  \\\n","    146   \n","    rouge_1  \\\n","      .1542   \n","    rouge_2  \\\n","     .04295   \n","    rouge_L  \\\n","      .1501   \n","    token_acc  \\\n","        .3469   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   102.1   \n","     tps  \n","   8.651\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8418)\n","02:23:49 | 0.4% complete (34 / 7,801), 0:06:53 elapsed, 1 day, 2:15:00 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   125.6   \n","    ctpb  \\\n","   88.26   \n","    ctps  \\\n","   7.473   \n","    ctrunc  \\\n","     .6471   \n","    ctrunclen  \\\n","        37.38   \n","     exps  \\\n","   .08466   \n","    exs  \\\n","     34   \n","      f1  \\\n","   .1624   \n","    gpu_mem  \\\n","     .09602   \n","    hits@1  \\\n","    .08824   \n","    hits@10  \\\n","      .6471   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4706   \n","    llen  \\\n","   14.06   \n","    loss  \\\n","   4.984   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   14.06   \n","    ltps  \\\n","    1.19   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","    ppl  \\\n","    146   \n","    rouge_1  \\\n","      .1562   \n","    rouge_2  \\\n","     .04169   \n","    rouge_L  \\\n","      .1522   \n","    token_acc  \\\n","        .3494   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   102.3   \n","     tps  \n","   8.663\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8500)\n","02:24:01 | 0.4% complete (35 / 7,801), 0:07:05 elapsed, 1 day, 2:14:48 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   126.9   \n","    ctpb  \\\n","    88.6   \n","    ctps  \\\n","   7.494   \n","    ctrunc  \\\n","     .6571   \n","    ctrunclen  \\\n","        38.31   \n","     exps  \\\n","   .08458   \n","    exs  \\\n","     35   \n","      f1  \\\n","   .1698   \n","    gpu_mem  \\\n","      .0960   \n","    hits@1  \\\n","    .08571   \n","    hits@10  \\\n","      .6571   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4857   \n","    llen  \\\n","   14.06   \n","    loss  \\\n","   4.958   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   14.06   \n","    ltps  \\\n","   1.189   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   142.3   \n","    rouge_1  \\\n","      .1631   \n","    rouge_2  \\\n","     .04367   \n","    rouge_L  \\\n","      .1593   \n","    token_acc  \\\n","        .3537   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   102.7   \n","     tps  \n","   8.682\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8464)\n","02:24:13 | 0.5% complete (36 / 7,801), 0:07:18 elapsed, 1 day, 2:14:45 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   128.7   \n","    ctpb  \\\n","   88.92   \n","    ctps  \\\n","   7.515   \n","    ctrunc  \\\n","     .6667   \n","    ctrunclen  \\\n","        39.75   \n","     exps  \\\n","   .08451   \n","    exs  \\\n","     36   \n","      f1  \\\n","   .1697   \n","    gpu_mem  \\\n","     .09597   \n","    hits@1  \\\n","    .08333   \n","    hits@10  \\\n","      .6667   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .5000   \n","    llen  \\\n","   13.86   \n","    loss  \\\n","    4.95   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   13.86   \n","    ltps  \\\n","   1.171   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   141.2   \n","    rouge_1  \\\n","      .1642   \n","    rouge_2  \\\n","     .04246   \n","    rouge_L  \\\n","      .1604   \n","    token_acc  \\\n","        .3527   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   102.8   \n","     tps  \n","   8.686\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8567)\n","02:24:26 | 0.5% complete (37 / 7,801), 0:07:30 elapsed, 1 day, 2:14:45 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   126.6   \n","    ctpb  \\\n","   87.95   \n","    ctps  \\\n","   7.426   \n","    ctrunc  \\\n","     .6486   \n","    ctrunclen  \\\n","        38.68   \n","     exps  \\\n","   .08444   \n","    exs  \\\n","     37   \n","      f1  \\\n","   .1723   \n","    gpu_mem  \\\n","     .09597   \n","    hits@1  \\\n","    .08108   \n","    hits@10  \\\n","      .6486   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4865   \n","    llen  \\\n","   13.76   \n","    loss  \\\n","   4.963   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   13.76   \n","    ltps  \\\n","   1.162   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","    ppl  \\\n","    143   \n","    rouge_1  \\\n","      .1665   \n","    rouge_2  \\\n","     .04517   \n","    rouge_L  \\\n","      .1629   \n","    token_acc  \\\n","        .3517   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   101.7   \n","     tps  \n","   8.588\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8117)\n","02:24:38 | 0.5% complete (38 / 7,801), 0:07:42 elapsed, 1 day, 2:14:53 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   125.3   \n","    ctpb  \\\n","   87.68   \n","    ctps  \\\n","   7.397   \n","    ctrunc  \\\n","     .6316   \n","    ctrunclen  \\\n","        37.66   \n","     exps  \\\n","   .08436   \n","    exs  \\\n","     38   \n","      f1  \\\n","   .1700   \n","    gpu_mem  \\\n","     .09606   \n","    hits@1  \\\n","    .07895   \n","    hits@10  \\\n","      .6579   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4737   \n","    llen  \\\n","   13.76   \n","    loss  \\\n","   4.971   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   13.76   \n","    ltps  \\\n","   1.161   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   144.2   \n","    rouge_1  \\\n","      .1647   \n","    rouge_2  \\\n","     .04398   \n","    rouge_L  \\\n","      .1612   \n","    token_acc  \\\n","        .3480   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   101.4   \n","     tps  \n","   8.559\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.7869)\n","02:24:50 | 0.5% complete (39 / 7,801), 0:07:54 elapsed, 1 day, 2:14:45 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   124.6   \n","    ctpb  \\\n","   87.92   \n","    ctps  \\\n","   7.411   \n","    ctrunc  \\\n","     .6154   \n","    ctrunclen  \\\n","        36.69   \n","     exps  \\\n","   .08429   \n","    exs  \\\n","     39   \n","      f1  \\\n","   .1656   \n","    gpu_mem  \\\n","     .09606   \n","    hits@1  \\\n","    .07692   \n","    hits@10  \\\n","      .6410   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4615   \n","    llen  \\\n","   13.67   \n","    loss  \\\n","   5.012   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   13.67   \n","    ltps  \\\n","   1.152   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   150.2   \n","    rouge_1  \\\n","      .1605   \n","    rouge_2  \\\n","     .04286   \n","    rouge_L  \\\n","      .1571   \n","    token_acc  \\\n","        .3433   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   101.6   \n","     tps  \n","   8.563\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8208)\n","02:25:03 | 0.5% complete (40 / 7,801), 0:08:07 elapsed, 1 day, 2:16:30 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   124.4   \n","    ctpb  \\\n","   88.22   \n","    ctps  \\\n","   7.431   \n","    ctrunc  \\\n","     .6250   \n","    ctrunclen  \\\n","        36.15   \n","     exps  \\\n","   .08423   \n","    exs  \\\n","     40   \n","      f1  \\\n","   .1615   \n","    gpu_mem  \\\n","     .09597   \n","    hits@1  \\\n","     .1000   \n","    hits@10  \\\n","      .6500   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4750   \n","    llen  \\\n","    13.5   \n","    loss  \\\n","   5.007   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","    13.5   \n","    ltps  \\\n","   1.137   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   149.5   \n","    rouge_1  \\\n","      .1565   \n","    rouge_2  \\\n","     .04179   \n","    rouge_L  \\\n","      .1532   \n","    token_acc  \\\n","        .3407   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   101.7   \n","     tps  \n","   8.569\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8529)\n","02:25:15 | 0.5% complete (41 / 7,801), 0:08:19 elapsed, 1 day, 2:16:25 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   124.6   \n","    ctpb  \\\n","   88.51   \n","    ctps  \\\n","   7.442   \n","    ctrunc  \\\n","     .6341   \n","    ctrunclen  \\\n","        36.07   \n","     exps  \\\n","   .08408   \n","    exs  \\\n","     41   \n","      f1  \\\n","   .1652   \n","    gpu_mem  \\\n","      .0960   \n","    hits@1  \\\n","    .09756   \n","    hits@10  \\\n","      .6341   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4634   \n","    llen  \\\n","   13.51   \n","    loss  \\\n","   5.035   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   13.51   \n","    ltps  \\\n","   1.136   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   153.7   \n","    rouge_1  \\\n","      .1600   \n","    rouge_2  \\\n","     .04348   \n","    rouge_L  \\\n","      .1567   \n","    token_acc  \\\n","        .3412   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","    tpb  \\\n","    102   \n","     tps  \n","   8.578\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8149)\n","02:25:27 | 0.5% complete (42 / 7,801), 0:08:32 elapsed, 1 day, 2:16:28 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   125.4   \n","    ctpb  \\\n","   88.79   \n","    ctps  \\\n","   7.459   \n","    ctrunc  \\\n","     .6429   \n","    ctrunclen  \\\n","        36.57   \n","     exps  \\\n","   .08402   \n","    exs  \\\n","     42   \n","      f1  \\\n","   .1646   \n","    gpu_mem  \\\n","     .09608   \n","    hits@1  \\\n","    .09524   \n","    hits@10  \\\n","      .6190   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4524   \n","    llen  \\\n","    13.6   \n","    loss  \\\n","   5.069   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","    13.6   \n","    ltps  \\\n","   1.142   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","    ppl  \\\n","    159   \n","    rouge_1  \\\n","      .1599   \n","    rouge_2  \\\n","     .04244   \n","    rouge_L  \\\n","      .1567   \n","    token_acc  \\\n","        .3363   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   102.4   \n","     tps  \n","   8.602\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.7997)\n","02:25:40 | 0.6% complete (43 / 7,801), 0:08:44 elapsed, 1 day, 2:16:23 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   126.9   \n","    ctpb  \\\n","   89.05   \n","    ctps  \\\n","   7.476   \n","    ctrunc  \\\n","     .6512   \n","    ctrunclen  \\\n","        37.84   \n","     exps  \\\n","   .08395   \n","    exs  \\\n","     43   \n","      f1  \\\n","   .1608   \n","    gpu_mem  \\\n","     .09608   \n","    hits@1  \\\n","     .1163   \n","    hits@10  \\\n","      .6279   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4651   \n","    llen  \\\n","   13.53   \n","    loss  \\\n","   5.008   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   13.53   \n","    ltps  \\\n","   1.136   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   149.6   \n","    rouge_1  \\\n","      .1561   \n","    rouge_2  \\\n","     .04145   \n","    rouge_L  \\\n","      .1530   \n","    token_acc  \\\n","        .3368   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   102.6   \n","     tps  \n","   8.612\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8134)\n","02:25:52 | 0.6% complete (44 / 7,801), 0:08:56 elapsed, 1 day, 2:16:57 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   125.2   \n","    ctpb  \\\n","    88.2   \n","    ctps  \\\n","   7.401   \n","    ctrunc  \\\n","     .6364   \n","    ctrunclen  \\\n","        36.98   \n","     exps  \\\n","   .08391   \n","    exs  \\\n","     44   \n","      f1  \\\n","   .1617   \n","    gpu_mem  \\\n","     .09603   \n","    hits@1  \\\n","     .1136   \n","    hits@10  \\\n","      .6364   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4773   \n","    llen  \\\n","   13.45   \n","    loss  \\\n","   4.999   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   13.45   \n","    ltps  \\\n","   1.129   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   148.3   \n","    rouge_1  \\\n","      .1583   \n","    rouge_2  \\\n","     .04376   \n","    rouge_L  \\\n","      .1552   \n","    token_acc  \\\n","        .3395   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   101.7   \n","    tps  \n","   8.53\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8137)\n","02:26:04 | 0.6% complete (45 / 7,801), 0:09:08 elapsed, 1 day, 2:16:04 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   124.1   \n","    ctpb  \\\n","   87.93   \n","    ctps  \\\n","   7.371   \n","    ctrunc  \\\n","     .6222   \n","    ctrunclen  \\\n","        36.16   \n","     exps  \\\n","   .08382   \n","    exs  \\\n","     45   \n","      f1  \\\n","   .1618   \n","    gpu_mem  \\\n","     .09601   \n","    hits@1  \\\n","     .1111   \n","    hits@10  \\\n","      .6222   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4667   \n","    llen  \\\n","   13.49   \n","    loss  \\\n","    5.03   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   13.49   \n","    ltps  \\\n","   1.131   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","    ppl  \\\n","    153   \n","    rouge_1  \\\n","      .1579   \n","    rouge_2  \\\n","     .04279   \n","    rouge_L  \\\n","      .1550   \n","    token_acc  \\\n","        .3361   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   101.4   \n","     tps  \n","   8.502\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8062)\n","02:26:16 | 0.6% complete (46 / 7,801), 0:09:20 elapsed, 1 day, 2:15:47 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   123.6   \n","    ctpb  \\\n","    88.2   \n","    ctps  \\\n","   7.392   \n","    ctrunc  \\\n","     .6304   \n","    ctrunclen  \\\n","        35.41   \n","     exps  \\\n","   .08382   \n","    exs  \\\n","     46   \n","      f1  \\\n","   .1603   \n","    gpu_mem  \\\n","     .09602   \n","    hits@1  \\\n","     .1087   \n","    hits@10  \\\n","      .6304   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4565   \n","    llen  \\\n","    13.5   \n","    loss  \\\n","   5.037   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","    13.5   \n","    ltps  \\\n","   1.132   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","    ppl  \\\n","    154   \n","    rouge_1  \\\n","      .1565   \n","    rouge_2  \\\n","     .04186   \n","    rouge_L  \\\n","      .1536   \n","    token_acc  \\\n","        .3333   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   101.7   \n","     tps  \n","   8.524\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8244)\n","02:26:28 | 0.6% complete (47 / 7,801), 0:09:32 elapsed, 1 day, 2:15:24 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   123.6   \n","    ctpb  \\\n","   88.45   \n","    ctps  \\\n","    7.41   \n","    ctrunc  \\\n","     .6383   \n","    ctrunclen  \\\n","        35.11   \n","     exps  \\\n","   .08378   \n","    exs  \\\n","     47   \n","      f1  \\\n","   .1589   \n","    gpu_mem  \\\n","     .09601   \n","    hits@1  \\\n","     .1064   \n","    hits@10  \\\n","      .6170   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4468   \n","    llen  \\\n","   13.49   \n","    loss  \\\n","   5.078   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   13.49   \n","    ltps  \\\n","    1.13   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   160.4   \n","    rouge_1  \\\n","      .1551   \n","    rouge_2  \\\n","     .04097   \n","    rouge_L  \\\n","      .1522   \n","    token_acc  \\\n","        .3265   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   101.9   \n","     tps  \n","   8.541\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8400)\n","02:26:40 | 0.6% complete (48 / 7,801), 0:09:45 elapsed, 1 day, 2:15:05 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","     124   \n","    ctpb  \\\n","   88.69   \n","    ctps  \\\n","   7.428   \n","    ctrunc  \\\n","     .6458   \n","    ctrunclen  \\\n","        35.27   \n","     exps  \\\n","   .08376   \n","    exs  \\\n","     48   \n","      f1  \\\n","   .1603   \n","    gpu_mem  \\\n","      .0960   \n","    hits@1  \\\n","     .1042   \n","    hits@10  \\\n","      .6042   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4375   \n","    llen  \\\n","   13.52   \n","    loss  \\\n","   5.118   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   13.52   \n","    ltps  \\\n","   1.132   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","    ppl  \\\n","    167   \n","    rouge_1  \\\n","      .1556   \n","    rouge_2  \\\n","     .04011   \n","    rouge_L  \\\n","      .1528   \n","    token_acc  \\\n","        .3220   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   102.2   \n","     tps  \n","   8.561\n","Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","tensor(0.8272)\n","02:26:53 | 0.6% complete (49 / 7,801), 0:09:57 elapsed, 1 day, 2:14:49 eta\n","    accuracy  \\\n","           0   \n","    clen  \\\n","   124.9   \n","    ctpb  \\\n","   88.92   \n","    ctps  \\\n","   7.445   \n","    ctrunc  \\\n","     .6531   \n","    ctrunclen  \\\n","           36   \n","     exps  \\\n","   .08373   \n","    exs  \\\n","     49   \n","      f1  \\\n","   .1623   \n","    gpu_mem  \\\n","     .09603   \n","    hits@1  \\\n","     .1020   \n","    hits@10  \\\n","      .5918   \n","    hits@100  \\\n","           1   \n","    hits@5  \\\n","     .4286   \n","    llen  \\\n","   13.61   \n","    loss  \\\n","   5.166   \n","    lr  \\\n","   1.2   \n","    ltpb  \\\n","   13.61   \n","    ltps  \\\n","    1.14   \n","    ltrunc  \\\n","         0   \n","    ltrunclen  \\\n","            0   \n","     ppl  \\\n","   175.2   \n","    rouge_1  \\\n","      .1563   \n","    rouge_2  \\\n","     .04065   \n","    rouge_L  \\\n","      .1535   \n","    token_acc  \\\n","        .3178   \n","    token_em  \\\n","           0   \n","    total_train_updates  \\\n","                   4854   \n","     tpb  \\\n","   102.5   \n","     tps  \n","   8.585\n"]}],"source":["!parlai eval_model -mf content/seq2seqmodel -t personachat --metrics rouge"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10240,"status":"ok","timestamp":1648417279098,"user":{"displayName":"Rushabh Shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04019200460865043006"},"user_tz":240},"id":"A0AkQ9OjUFAe","outputId":"da88da1b-8769-438f-c4c9-bc47f9b87ca5"},"outputs":[{"name":"stdout","output_type":"stream","text":["fatal: destination path 'bleurt' already exists and is not an empty directory.\n","/bleurt\n","Processing /bleurt\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from BLEURT==0.0.2) (1.3.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from BLEURT==0.0.2) (1.21.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from BLEURT==0.0.2) (1.4.1)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from BLEURT==0.0.2) (2.8.0)\n","Collecting tf-slim>=1.1\n","  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.1/352.1 KB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from tf-slim>=1.1->BLEURT==0.0.2) (1.0.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->BLEURT==0.0.2) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->BLEURT==0.0.2) (2.8.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (1.44.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (1.15.0)\n","Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (2.8.0)\n","Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (2.8.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (3.1.0)\n","Collecting tf-estimator-nightly==2.8.0.dev2021122109\n","  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.5/462.5 KB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (1.1.0)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (13.0.0)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (2.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (1.1.2)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (0.24.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (57.4.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (1.14.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (1.6.3)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (3.10.0.2)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (3.17.3)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (0.2.0)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (0.5.3)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->BLEURT==0.0.2) (3.3.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->BLEURT==0.0.2) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->BLEURT==0.0.2) (1.5.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->BLEURT==0.0.2) (1.35.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->BLEURT==0.0.2) (1.8.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->BLEURT==0.0.2) (0.4.6)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->BLEURT==0.0.2) (2.27.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->BLEURT==0.0.2) (0.6.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->BLEURT==0.0.2) (3.3.4)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow->BLEURT==0.0.2) (1.0.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->BLEURT==0.0.2) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->BLEURT==0.0.2) (4.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->BLEURT==0.0.2) (4.2.4)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->BLEURT==0.0.2) (1.3.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->BLEURT==0.0.2) (4.2.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->BLEURT==0.0.2) (1.26.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->BLEURT==0.0.2) (2021.10.8)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->BLEURT==0.0.2) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow->BLEURT==0.0.2) (2.10)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow->BLEURT==0.0.2) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow->BLEURT==0.0.2) (3.2.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow->BLEURT==0.0.2) (3.7.0)\n","Building wheels for collected packages: BLEURT\n","  Building wheel for BLEURT (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for BLEURT: filename=BLEURT-0.0.2-py3-none-any.whl size=16456761 sha256=dd9bf21c16646f518eae8ddae1904b6d7a0aeb641c77515c7bed6b303c34b245\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-n7zbpwni/wheels/cc/56/4e/55a97c558c25ba677da0d073f149df2568a430dd55665201bf\n","Successfully built BLEURT\n","Installing collected packages: tf-estimator-nightly, sentencepiece, tf-slim, BLEURT\n","Successfully installed BLEURT-0.0.2 sentencepiece-0.1.96 tf-estimator-nightly-2.8.0.dev2021122109 tf-slim-1.1.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!git clone https://github.com/google-research/bleurt.git\n","%cd bleurt\n","!pip install ."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":99044,"status":"ok","timestamp":1648417815547,"user":{"displayName":"Rushabh Shah","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04019200460865043006"},"user_tz":240},"id":"c9e-lA-zWWGR","outputId":"f2cd7ece-6d04-45f8-d746-e8a86b285ab0"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2022-03-27 21:48:36--  https://storage.googleapis.com/bleurt-oss-21/BLEURT-20.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.140.128, 108.177.15.128, 173.194.76.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.140.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2140294207 (2.0G) [application/octet-stream]\n","Saving to: ‘BLEURT-20.zip’\n","\n","BLEURT-20.zip       100%[===================>]   1.99G  49.1MB/s    in 44s     \n","\n","2022-03-27 21:49:21 (46.6 MB/s) - ‘BLEURT-20.zip’ saved [2140294207/2140294207]\n","\n","--2022-03-27 21:49:21--  http://./\n","Resolving . (.)... failed: No address associated with hostname.\n","wget: unable to resolve host address ‘.’\n","FINISHED --2022-03-27 21:49:21--\n","Total wall clock time: 45s\n","Downloaded: 1 files, 2.0G in 44s (46.6 MB/s)\n","Archive:  BLEURT-20.zip\n","   creating: BLEURT-20/\n","  inflating: BLEURT-20/bert_config.json  \n","  inflating: BLEURT-20/saved_model.pb  \n","   creating: BLEURT-20/variables/\n","  inflating: BLEURT-20/variables/variables.index  \n","  inflating: BLEURT-20/variables/variables.data-00000-of-00001  \n","  inflating: BLEURT-20/sent_piece.vocab  \n","  inflating: BLEURT-20/bleurt_config.json  \n","  inflating: BLEURT-20/sent_piece.model  \n","INFO:tensorflow:Running BLEURT scoring.\n","I0327 21:49:52.894911 139736428840832 score_files.py:169] Running BLEURT scoring.\n","INFO:tensorflow:Reading checkpoint BLEURT-20.\n","I0327 21:49:52.895255 139736428840832 score.py:161] Reading checkpoint BLEURT-20.\n","INFO:tensorflow:Config file found, reading.\n","I0327 21:49:52.895705 139736428840832 checkpoint.py:92] Config file found, reading.\n","INFO:tensorflow:Will load checkpoint BLEURT-20\n","I0327 21:49:52.899582 139736428840832 checkpoint.py:96] Will load checkpoint BLEURT-20\n","INFO:tensorflow:Loads full paths and checks that files exists.\n","I0327 21:49:52.899746 139736428840832 checkpoint.py:98] Loads full paths and checks that files exists.\n","INFO:tensorflow:... name:BLEURT-20\n","I0327 21:49:52.899907 139736428840832 checkpoint.py:102] ... name:BLEURT-20\n","INFO:tensorflow:... bert_config_file:bert_config.json\n","I0327 21:49:52.900066 139736428840832 checkpoint.py:102] ... bert_config_file:bert_config.json\n","INFO:tensorflow:... max_seq_length:512\n","I0327 21:49:52.900227 139736428840832 checkpoint.py:102] ... max_seq_length:512\n","INFO:tensorflow:... vocab_file:None\n","I0327 21:49:52.900363 139736428840832 checkpoint.py:102] ... vocab_file:None\n","INFO:tensorflow:... do_lower_case:None\n","I0327 21:49:52.900483 139736428840832 checkpoint.py:102] ... do_lower_case:None\n","INFO:tensorflow:... sp_model:sent_piece\n","I0327 21:49:52.900624 139736428840832 checkpoint.py:102] ... sp_model:sent_piece\n","INFO:tensorflow:... dynamic_seq_length:True\n","I0327 21:49:52.900788 139736428840832 checkpoint.py:102] ... dynamic_seq_length:True\n","INFO:tensorflow:Creating BLEURT scorer.\n","I0327 21:49:52.900998 139736428840832 score.py:168] Creating BLEURT scorer.\n","INFO:tensorflow:Creating SentencePiece tokenizer.\n","I0327 21:49:52.901118 139736428840832 tokenizers.py:79] Creating SentencePiece tokenizer.\n","INFO:tensorflow:Creating SentencePiece tokenizer.\n","I0327 21:49:52.901250 139736428840832 tokenizers.py:58] Creating SentencePiece tokenizer.\n","INFO:tensorflow:Will load model: BLEURT-20/sent_piece.model.\n","I0327 21:49:52.901371 139736428840832 tokenizers.py:60] Will load model: BLEURT-20/sent_piece.model.\n","INFO:tensorflow:SentencePiece tokenizer created.\n","I0327 21:49:53.457525 139736428840832 tokenizers.py:64] SentencePiece tokenizer created.\n","INFO:tensorflow:Creating Eager Mode predictor.\n","I0327 21:49:53.457811 139736428840832 score.py:57] Creating Eager Mode predictor.\n","INFO:tensorflow:Loading model.\n","I0327 21:49:53.457956 139736428840832 score.py:62] Loading model.\n","2022-03-27 21:49:58.415438: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n","INFO:tensorflow:BLEURT initialized.\n","I0327 21:50:07.508165 139736428840832 score.py:174] BLEURT initialized.\n","INFO:tensorflow:Computing BLEURT scores...\n","I0327 21:50:07.508491 139736428840832 score_files.py:133] Computing BLEURT scores...\n","INFO:tensorflow:BLEURT scores computed.\n","I0327 21:50:13.098470 139736428840832 score_files.py:141] BLEURT scores computed.\n","0.9777965545654297\n","0.7689982652664185\n","0.4400973916053772\n","0.16677378118038177\n","INFO:tensorflow:Done.\n","I0327 21:50:13.098881 139736428840832 score_files.py:151] Done.\n"]}],"source":["!wget https://storage.googleapis.com/bleurt-oss-21/BLEURT-20.zip .\n","!unzip BLEURT-20.zip\n","\n","# Runs the scoring.\n","!python -m bleurt.score_files \\\n","  -candidate_file=bleurt/test_data/candidates \\\n","  -reference_file=bleurt/test_data/references \\\n","  -bleurt_checkpoint=BLEURT-20"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"NLP Persona Based Social Bot Seq-2-Seq.ipynb","provenance":[{"file_id":"16q62f5UmXpDRzTusHQkP2qa8fqmCl3hr","timestamp":1648235038349}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}